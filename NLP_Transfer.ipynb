{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Transfer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfftZAOgdn9TwQIPgpYbwh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chunbo777/twig_farm_nlp/blob/main/NLP_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIUoVxfeMJIU",
        "outputId": "b8e6c47e-eb82-4dc6-9c32-5ab1823ede0d"
      },
      "source": [
        "!git clone https://github.com/chunbo777/NLP_text-style-transfer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP_text-style-transfer'...\n",
            "remote: Enumerating objects: 605, done.\u001b[K\n",
            "remote: Counting objects: 100% (605/605), done.\u001b[K\n",
            "remote: Compressing objects: 100% (242/242), done.\u001b[K\n",
            "remote: Total 605 (delta 358), reused 598 (delta 355), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (605/605), 37.00 MiB | 20.57 MiB/s, done.\n",
            "Resolving deltas: 100% (358/358), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TElRc2OLEp6x",
        "outputId": "4f805dfa-3c2a-4b8e-f340-dba591483e86"
      },
      "source": [
        "%cd /content/NLP_text-style-transfer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NLP_text-style-transfer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjx_qtHsmW9A",
        "outputId": "b57636e3-d77a-48b7-b843-10a9946c04b4"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting pandas==1.1.2\n",
            "  Downloading pandas-1.1.2-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5 MB 41.3 MB/s \n",
            "\u001b[?25hCollecting scipy==1.5.2\n",
            "  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.7 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 45.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow_hub==0.9.0\n",
            "  Downloading tensorflow_hub-0.9.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 74.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow_text==2.3.0\n",
            "  Downloading tensorflow_text-2.3.0-cp37-cp37m-manylinux1_x86_64.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 49.4 MB/s \n",
            "\u001b[?25hCollecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 15 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.49.0\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting transformers==3.2.0\n",
            "  Downloading transformers-3.2.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 39.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.2->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.2->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub==0.9.0->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub==0.9.0->-r requirements.txt (line 5)) (3.17.3)\n",
            "Collecting tensorflow<2.4,>=2.3.0\n",
            "  Downloading tensorflow-2.3.4-cp37-cp37m-manylinux2010_x86_64.whl (320.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.6 MB 53 kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r requirements.txt (line 7)) (0.16.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 28.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0->-r requirements.txt (line 9)) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0->-r requirements.txt (line 9)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0->-r requirements.txt (line 9)) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.2.0->-r requirements.txt (line 9)) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 40.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow<2.4,>=2.3.0\n",
            "  Downloading tensorflow-2.3.3-cp37-cp37m-manylinux2010_x86_64.whl (320.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.5 MB 14 kB/s \n",
            "\u001b[?25h  Downloading tensorflow-2.3.2-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 11 kB/s \n",
            "\u001b[?25h  Downloading tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 22 kB/s \n",
            "\u001b[?25h  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 47 kB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of six to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting six>=1.12.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
            "  Downloading six-1.13.0-py2.py3-none-any.whl (10 kB)\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "INFO: pip is looking at multiple versions of regex to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2021.8.28-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 43.1 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of six to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading regex-2021.8.21-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (747 kB)\n",
            "\u001b[K     |████████████████████████████████| 747 kB 72.3 MB/s \n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading regex-2021.8.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 52.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2021.7.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 62.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2021.7.5-cp37-cp37m-manylinux2014_x86_64.whl (721 kB)\n",
            "\u001b[K     |████████████████████████████████| 721 kB 21.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2021.7.1-cp37-cp37m-manylinux2014_x86_64.whl (721 kB)\n",
            "\u001b[K     |████████████████████████████████| 721 kB 70.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720 kB)\n",
            "\u001b[K     |████████████████████████████████| 720 kB 66.7 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of regex to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading regex-2021.3.17-cp37-cp37m-manylinux2014_x86_64.whl (721 kB)\n",
            "\u001b[K     |████████████████████████████████| 721 kB 57.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.11.13-cp37-cp37m-manylinux2014_x86_64.whl (719 kB)\n",
            "\u001b[K     |████████████████████████████████| 719 kB 59.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.11.11-cp37-cp37m-manylinux2014_x86_64.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 72.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.10.28-cp37-cp37m-manylinux2014_x86_64.whl (721 kB)\n",
            "\u001b[K     |████████████████████████████████| 721 kB 72.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.10.23-cp37-cp37m-manylinux2010_x86_64.whl (662 kB)\n",
            "\u001b[K     |████████████████████████████████| 662 kB 58.8 MB/s \n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading regex-2020.10.22-cp37-cp37m-manylinux2010_x86_64.whl (662 kB)\n",
            "\u001b[K     |████████████████████████████████| 662 kB 58.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.10.15-cp37-cp37m-manylinux2010_x86_64.whl (662 kB)\n",
            "\u001b[K     |████████████████████████████████| 662 kB 45.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.10.11-cp37-cp37m-manylinux2010_x86_64.whl (662 kB)\n",
            "\u001b[K     |████████████████████████████████| 662 kB 74.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.9.27-cp37-cp37m-manylinux2010_x86_64.whl (662 kB)\n",
            "\u001b[K     |████████████████████████████████| 662 kB 74.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.7.14-cp37-cp37m-manylinux2010_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 64.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.6.8-cp37-cp37m-manylinux2010_x86_64.whl (661 kB)\n",
            "\u001b[K     |████████████████████████████████| 661 kB 71.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.6.7-cp37-cp37m-manylinux2010_x86_64.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 48.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.5.14-cp37-cp37m-manylinux2010_x86_64.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 72.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.5.13-cp37-cp37m-manylinux2010_x86_64.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 30.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.5.7-cp37-cp37m-manylinux2010_x86_64.whl (676 kB)\n",
            "\u001b[K     |████████████████████████████████| 676 kB 69.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.4.4-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 53.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.2.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 43.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.2.18-cp37-cp37m-manylinux2010_x86_64.whl (690 kB)\n",
            "\u001b[K     |████████████████████████████████| 690 kB 58.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.1.8-cp37-cp37m-manylinux2010_x86_64.whl (690 kB)\n",
            "\u001b[K     |████████████████████████████████| 690 kB 76.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2020.1.7-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 69.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 73.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.12.19-cp37-cp37m-manylinux2010_x86_64.whl (688 kB)\n",
            "\u001b[K     |████████████████████████████████| 688 kB 59.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.12.18-cp37-cp37m-manylinux2010_x86_64.whl (688 kB)\n",
            "\u001b[K     |████████████████████████████████| 688 kB 39.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.12.9.tar.gz (669 kB)\n",
            "\u001b[K     |████████████████████████████████| 669 kB 48.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.11.1.tar.gz (669 kB)\n",
            "\u001b[K     |████████████████████████████████| 669 kB 53.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.08.19.tar.gz (654 kB)\n",
            "\u001b[K     |████████████████████████████████| 654 kB 66.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.06.08.tar.gz (651 kB)\n",
            "\u001b[K     |████████████████████████████████| 651 kB 64.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.06.05.tar.gz (651 kB)\n",
            "\u001b[K     |████████████████████████████████| 651 kB 43.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.06.02.tar.gz (651 kB)\n",
            "\u001b[K     |████████████████████████████████| 651 kB 59.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.05.25.tar.gz (645 kB)\n",
            "\u001b[K     |████████████████████████████████| 645 kB 52.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.04.14.tar.gz (644 kB)\n",
            "\u001b[K     |████████████████████████████████| 644 kB 36.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.04.12.tar.gz (644 kB)\n",
            "\u001b[K     |████████████████████████████████| 644 kB 49.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.04.10.tar.gz (644 kB)\n",
            "\u001b[K     |████████████████████████████████| 644 kB 33.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.04.09.tar.gz (644 kB)\n",
            "\u001b[K     |████████████████████████████████| 644 kB 47.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.03.12.tar.gz (644 kB)\n",
            "\u001b[K     |████████████████████████████████| 644 kB 54.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.03.09.tar.gz (643 kB)\n",
            "\u001b[K     |████████████████████████████████| 643 kB 35.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.03.08.tar.gz (643 kB)\n",
            "\u001b[K     |████████████████████████████████| 643 kB 47.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.02.21.tar.gz (643 kB)\n",
            "\u001b[K     |████████████████████████████████| 643 kB 48.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.02.20.tar.gz (713 kB)\n",
            "\u001b[K     |████████████████████████████████| 713 kB 64.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.02.19.tar.gz (643 kB)\n",
            "\u001b[K     |████████████████████████████████| 643 kB 56.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.02.18.tar.gz (643 kB)\n",
            "\u001b[K     |████████████████████████████████| 643 kB 53.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.02.07.tar.gz (643 kB)\n",
            "\u001b[K     |████████████████████████████████| 643 kB 64.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.02.06.tar.gz (643 kB)\n",
            "\u001b[K     |████████████████████████████████| 643 kB 54.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.02.05.tar.gz (642 kB)\n",
            "\u001b[K     |████████████████████████████████| 642 kB 57.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.02.03.tar.gz (643 kB)\n",
            "\u001b[K     |████████████████████████████████| 643 kB 54.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.01.24.tar.gz (647 kB)\n",
            "\u001b[K     |████████████████████████████████| 647 kB 35.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2019.01.23.tar.gz (647 kB)\n",
            "\u001b[K     |████████████████████████████████| 647 kB 56.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.11.22.tar.gz (648 kB)\n",
            "\u001b[K     |████████████████████████████████| 648 kB 27.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.11.07.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 54.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.11.06.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 37.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.11.03.tar.gz (644 kB)\n",
            "\u001b[K     |████████████████████████████████| 644 kB 61.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.11.02.tar.gz (644 kB)\n",
            "\u001b[K     |████████████████████████████████| 644 kB 55.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.08.29.tar.gz (643 kB)\n",
            "\u001b[K     |████████████████████████████████| 643 kB 1.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.08.17.tar.gz (643 kB)\n",
            "\u001b[K     |████████████████████████████████| 643 kB 46.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.07.11.tar.gz (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 50.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.06.21.tar.gz (632 kB)\n",
            "\u001b[K     |████████████████████████████████| 632 kB 43.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.06.20.tar.gz (632 kB)\n",
            "\u001b[K     |████████████████████████████████| 632 kB 64.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.06.09.tar.gz (632 kB)\n",
            "\u001b[K     |████████████████████████████████| 632 kB 46.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.06.06.tar.gz (627 kB)\n",
            "\u001b[K     |████████████████████████████████| 627 kB 60.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.02.21.tar.gz (620 kB)\n",
            "\u001b[K     |████████████████████████████████| 620 kB 65.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.02.08.tar.gz (620 kB)\n",
            "\u001b[K     |████████████████████████████████| 620 kB 66.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.02.03.tar.gz (620 kB)\n",
            "\u001b[K     |████████████████████████████████| 620 kB 48.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2018.01.10.tar.gz (612 kB)\n",
            "\u001b[K     |████████████████████████████████| 612 kB 68.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.12.12.tar.gz (612 kB)\n",
            "\u001b[K     |████████████████████████████████| 612 kB 28.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.12.09.tar.gz (612 kB)\n",
            "\u001b[K     |████████████████████████████████| 612 kB 55.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.12.05.tar.gz (612 kB)\n",
            "\u001b[K     |████████████████████████████████| 612 kB 65.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.11.09.tar.gz (608 kB)\n",
            "\u001b[K     |████████████████████████████████| 608 kB 62.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.11.08.tar.gz (608 kB)\n",
            "\u001b[K     |████████████████████████████████| 608 kB 51.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.09.23.tar.gz (607 kB)\n",
            "\u001b[K     |████████████████████████████████| 607 kB 67.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.07.28.tar.gz (607 kB)\n",
            "\u001b[K     |████████████████████████████████| 607 kB 53.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.07.26.tar.gz (607 kB)\n",
            "\u001b[K     |████████████████████████████████| 607 kB 63.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.07.11.tar.gz (607 kB)\n",
            "\u001b[K     |████████████████████████████████| 607 kB 52.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.06.23.tar.gz (607 kB)\n",
            "\u001b[K     |████████████████████████████████| 607 kB 52.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.06.20.tar.gz (603 kB)\n",
            "\u001b[K     |████████████████████████████████| 603 kB 65.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.06.07.tar.gz (603 kB)\n",
            "\u001b[K     |████████████████████████████████| 603 kB 53.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.05.26.tar.gz (602 kB)\n",
            "\u001b[K     |████████████████████████████████| 602 kB 60.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.04.29.tar.gz (602 kB)\n",
            "\u001b[K     |████████████████████████████████| 602 kB 61.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.04.23.tar.gz (602 kB)\n",
            "\u001b[K     |████████████████████████████████| 602 kB 43.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.04.05.tar.gz (601 kB)\n",
            "\u001b[K     |████████████████████████████████| 601 kB 65.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.02.08.tar.gz (600 kB)\n",
            "\u001b[K     |████████████████████████████████| 600 kB 67.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.01.17.tar.gz (601 kB)\n",
            "\u001b[K     |████████████████████████████████| 601 kB 64.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.01.14.tar.gz (600 kB)\n",
            "\u001b[K     |████████████████████████████████| 600 kB 54.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2017.01.12.tar.gz (599 kB)\n",
            "\u001b[K     |████████████████████████████████| 599 kB 48.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.12.27.tar.gz (599 kB)\n",
            "\u001b[K     |████████████████████████████████| 599 kB 61.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.11.21.tar.gz (599 kB)\n",
            "\u001b[K     |████████████████████████████████| 599 kB 64.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.11.18.tar.gz (599 kB)\n",
            "\u001b[K     |████████████████████████████████| 599 kB 54.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.10.22.tar.gz (599 kB)\n",
            "\u001b[K     |████████████████████████████████| 599 kB 62.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.09.22.tar.gz (599 kB)\n",
            "\u001b[K     |████████████████████████████████| 599 kB 62.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.08.27.tar.gz (598 kB)\n",
            "\u001b[K     |████████████████████████████████| 598 kB 68.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.07.21.tar.gz (598 kB)\n",
            "\u001b[K     |████████████████████████████████| 598 kB 62.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.07.14.tar.gz (598 kB)\n",
            "\u001b[K     |████████████████████████████████| 598 kB 48.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.06.24.tar.gz (598 kB)\n",
            "\u001b[K     |████████████████████████████████| 598 kB 46.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.06.19.tar.gz (584 kB)\n",
            "\u001b[K     |████████████████████████████████| 584 kB 65.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.06.14.tar.gz (584 kB)\n",
            "\u001b[K     |████████████████████████████████| 584 kB 64.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.06.05.tar.gz (581 kB)\n",
            "\u001b[K     |████████████████████████████████| 581 kB 54.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.06.02.tar.gz (581 kB)\n",
            "\u001b[K     |████████████████████████████████| 581 kB 52.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.05.23.tar.gz (580 kB)\n",
            "\u001b[K     |████████████████████████████████| 580 kB 62.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.05.15.tar.gz (580 kB)\n",
            "\u001b[K     |████████████████████████████████| 580 kB 54.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.05.14.tar.gz (580 kB)\n",
            "\u001b[K     |████████████████████████████████| 580 kB 51.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.05.13.tar.gz (580 kB)\n",
            "\u001b[K     |████████████████████████████████| 580 kB 45.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.04.25.tar.gz (579 kB)\n",
            "\u001b[K     |████████████████████████████████| 579 kB 41.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.04.15.tar.gz (579 kB)\n",
            "\u001b[K     |████████████████████████████████| 579 kB 43.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.04.08.tar.gz (579 kB)\n",
            "\u001b[K     |████████████████████████████████| 579 kB 36.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.04.03.tar.gz (579 kB)\n",
            "\u001b[K     |████████████████████████████████| 579 kB 50.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.04.02.tar.gz (578 kB)\n",
            "\u001b[K     |████████████████████████████████| 578 kB 63.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.04.01.tar.gz (577 kB)\n",
            "\u001b[K     |████████████████████████████████| 577 kB 54.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.03.31.tar.gz (576 kB)\n",
            "\u001b[K     |████████████████████████████████| 576 kB 48.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.03.26.tar.gz (576 kB)\n",
            "\u001b[K     |████████████████████████████████| 576 kB 53.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.03.24.tar.gz (576 kB)\n",
            "\u001b[K     |████████████████████████████████| 576 kB 65.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.03.02.tar.gz (575 kB)\n",
            "\u001b[K     |████████████████████████████████| 575 kB 62.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.02.25.tar.gz (575 kB)\n",
            "\u001b[K     |████████████████████████████████| 575 kB 52.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.02.24.tar.gz (575 kB)\n",
            "\u001b[K     |████████████████████████████████| 575 kB 53.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.02.23.tar.gz (575 kB)\n",
            "\u001b[K     |████████████████████████████████| 575 kB 61.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2016.01.10.tar.gz (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 55.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.11.22.tar.gz (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 34.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.11.14.tar.gz (572 kB)\n",
            "\u001b[K     |████████████████████████████████| 572 kB 46.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.11.12.tar.gz (572 kB)\n",
            "\u001b[K     |████████████████████████████████| 572 kB 53.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.11.09.tar.gz (572 kB)\n",
            "\u001b[K     |████████████████████████████████| 572 kB 55.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.11.08.tar.gz (572 kB)\n",
            "\u001b[K     |████████████████████████████████| 572 kB 54.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.11.07.tar.gz (572 kB)\n",
            "\u001b[K     |████████████████████████████████| 572 kB 67.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.10.29.tar.gz (568 kB)\n",
            "\u001b[K     |████████████████████████████████| 568 kB 53.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.10.22.tar.gz (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 63.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.10.05.tar.gz (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 61.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.10.01.tar.gz (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 50.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.09.28.tar.gz (563 kB)\n",
            "\u001b[K     |████████████████████████████████| 563 kB 47.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.09.23.tar.gz (563 kB)\n",
            "\u001b[K     |████████████████████████████████| 563 kB 46.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.09.15.tar.gz (563 kB)\n",
            "\u001b[K     |████████████████████████████████| 563 kB 44.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.09.14.tar.gz (559 kB)\n",
            "\u001b[K     |████████████████████████████████| 559 kB 65.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.07.19.tar.gz (554 kB)\n",
            "\u001b[K     |████████████████████████████████| 554 kB 63.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.07.12.tar.gz (554 kB)\n",
            "\u001b[K     |████████████████████████████████| 554 kB 55.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.06.24.tar.gz (553 kB)\n",
            "\u001b[K     |████████████████████████████████| 553 kB 62.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.06.21.tar.gz (553 kB)\n",
            "\u001b[K     |████████████████████████████████| 553 kB 54.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.06.19.tar.gz (553 kB)\n",
            "\u001b[K     |████████████████████████████████| 553 kB 49.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.06.15.tar.gz (547 kB)\n",
            "\u001b[K     |████████████████████████████████| 547 kB 67.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.06.14.tar.gz (547 kB)\n",
            "\u001b[K     |████████████████████████████████| 547 kB 47.8 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.06.10.tar.gz (547 kB)\n",
            "\u001b[K     |████████████████████████████████| 547 kB 46.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.06.09.tar.gz (547 kB)\n",
            "\u001b[K     |████████████████████████████████| 547 kB 53.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.06.04.tar.gz (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 54.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.06.02.tar.gz (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 54.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.05.28.tar.gz (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 66.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.05.10.tar.gz (538 kB)\n",
            "\u001b[K     |████████████████████████████████| 538 kB 63.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.05.07.tar.gz (539 kB)\n",
            "\u001b[K     |████████████████████████████████| 539 kB 67.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2015.03.18.tar.gz (538 kB)\n",
            "\u001b[K     |████████████████████████████████| 538 kB 66.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.12.24.tar.gz (534 kB)\n",
            "\u001b[K     |████████████████████████████████| 534 kB 58.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.12.15.tar.gz (534 kB)\n",
            "\u001b[K     |████████████████████████████████| 534 kB 40.4 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.11.14.tar.gz (534 kB)\n",
            "\u001b[K     |████████████████████████████████| 534 kB 33.6 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.11.13.tar.gz (534 kB)\n",
            "\u001b[K     |████████████████████████████████| 534 kB 65.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.11.03.tar.gz (534 kB)\n",
            "\u001b[K     |████████████████████████████████| 534 kB 52.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.10.24.tar.gz (534 kB)\n",
            "\u001b[K     |████████████████████████████████| 534 kB 68.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.10.23.tar.gz (537 kB)\n",
            "\u001b[K     |████████████████████████████████| 537 kB 53.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.10.09.tar.gz (534 kB)\n",
            "\u001b[K     |████████████████████████████████| 534 kB 47.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.10.07.tar.gz (533 kB)\n",
            "\u001b[K     |████████████████████████████████| 533 kB 52.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.10.02.tar.gz (533 kB)\n",
            "\u001b[K     |████████████████████████████████| 533 kB 60.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.10.01.tar.gz (533 kB)\n",
            "\u001b[K     |████████████████████████████████| 533 kB 42.1 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.09.22.tar.gz (533 kB)\n",
            "\u001b[K     |████████████████████████████████| 533 kB 61.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.09.18.tar.gz (532 kB)\n",
            "\u001b[K     |████████████████████████████████| 532 kB 49.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.08.28.tar.gz (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 60.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.08.15.tar.gz (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 42.7 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.06.28.tar.gz (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 37.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.05.23.tar.gz (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 45.2 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.05.17.tar.gz (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 40.9 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.04.10.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 36.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.02.19.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 37.0 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.02.16.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 54.3 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.01.30.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 52.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.01.20.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 51.5 MB/s \n",
            "\u001b[?25h  Downloading regex-2014.01.10.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 43.1 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20130125.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.2 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20130124.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 45.7 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20130120.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 41.6 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20121216.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 65.3 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20121120.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 42.0 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20121113.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 60.5 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20121105.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.4 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20121031.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.5 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20121017.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 66.6 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20121008.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 65.8 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20120904.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 43.7 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20120825.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 27.8 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20120803.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 59.3 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20120710.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.9 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20120709.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.5 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20120708.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 46.5 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20120705.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 42.1 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20120613.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 63.6 MB/s \n",
            "\u001b[?25h  Downloading regex-0.1.20120611.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 36.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/76/bf/a55d40949db1767a6a342d17e672929070e93effe12505e657a954c07d44/regex-0.1.20120611.tar.gz#sha256=b931bb5604ad4f965709382a27b7ea76250d94ddd0006277134b1a75c2693704 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120506.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 63.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/56/2c/25922c63a96605ebce9aaac4a2012b7f8cad88d4b0b8b2b1a6bac7c99eb1/regex-0.1.20120506.tar.gz#sha256=f3243f10336cb9625bcd1d06a201854d915287700e4e4668401f4335c8de056f (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120504.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 61.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/66/58/a978f7b9051faa6ebe6b3c6839c11156c5f5ed7e204d1ea08bf61da2d0c0/regex-0.1.20120504.tar.gz#sha256=ac1616c60bb7369750adaf97e8d2189603c569b06089685df739663a448742ba (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120503.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 36.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/3a/2d/0fef9473a4107389924a5bd911c524db2045e8fcf39e15318d492cec06b0/regex-0.1.20120503.tar.gz#sha256=9c8d8e323694aa7aa149d347aa70fd745cf7f9c21bd4ded6cbd7572003f5e451 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120502.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 25.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/15/5c/cf5aca7d90770028bd6c17a86da3ed66f89bc683db29a5b154df2e04e81d/regex-0.1.20120502.tar.gz#sha256=2a31e528fb911447e33ffcc669121d26e9a545a9a90bf135c7e1a7d82ead3046 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120416.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 52.5 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/27/e9/aa1f669eaa752f4cf365a38468754b17b4ef9a3e4ef6450ce27f34b21a0b/regex-0.1.20120416.tar.gz#sha256=dc24a45625cccf214e2e760ad1453a08176c81ae62561315c04512efc2809fd8 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120323.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 47.0 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/48/2a/6d0249e812adf74fde0f134e449c71e9742272f68d09dd31a1eb79a5a67d/regex-0.1.20120323.tar.gz#sha256=209cf314c3368eb24d6e7e0529eb483aa577337db414392eccbda00fbf3a34a5 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120317.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 23.2 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/11/e6/1b3e2984674bf055efc45d451aae76fcb3b3d94e70d275dfaa0cb31e4945/regex-0.1.20120317.tar.gz#sha256=bdb4812d8a1d4ee5eec8dd9a97cc96ef7576d34d4ac24bd57142f420795bb62c (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120316.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 51.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/31/ab/46b8d3bd742bcc44f5119d3581cb237d1f65a1e308b9bcf694557f43441f/regex-0.1.20120316.tar.gz#sha256=d913c2ef780537b4ef7dfffc3fe957caa80cfd6484d6483f87cdfdec39af66cf (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120303.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/5c/74/fa3c7a27732685ae0cc2f866630f0df1aafca3825dee3ff23d0add8898fb/regex-0.1.20120303.tar.gz#sha256=2e38b95619f93f6b6f80b50344b04226f703dc5a52dd7c53338233ae501602cb (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120301.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 42.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/51/09/7ad4f68dc59cb62cafc5f47b2c32c2e6cd9ca2b7fb3a99d7483490242718/regex-0.1.20120301.tar.gz#sha256=db7bdd33b5112f59f1ea4a6eca4c4c2a72afdf5813eff81d1af4ae6301a4a3bc (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120209.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 45.5 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/a1/bb/a30af135ba3f2a76f23885b0d90c01af829dd4b52cfb49815e626e284fc7/regex-0.1.20120209.tar.gz#sha256=502d9d48e0314c0f9eded97c0071a4f9b05f680b6aa3f52776e578aae4532c28 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120208.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 47.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/e9/6f/9c19182eb95f39dde1e43c7261b19c952d0cbe60a800096120c21a9590d5/regex-0.1.20120208.tar.gz#sha256=00fa349854971cc19d97ea309d0c70161e13475a3198f1204b6710318f0e1917 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120129.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 48.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/89/f9/6e480ae5609adebdb8239fd10fa15756951416d530e813f9c87cd19b7305/regex-0.1.20120129.tar.gz#sha256=bc2fbe64160cae30641d337e81aa54a4ad5d4cf433b41dbd57fa64b35c8ae85f (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120128.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 58.2 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/93/49/6aaf924268fa85fbfa29040ef9829f32238df044ecc9639ca88e38e01a23/regex-0.1.20120128.tar.gz#sha256=64fa1247ccb2988ef2228dc02e2930a90e57fd06fe090305d3b322724bcbdf30 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120126.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 42.0 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/de/24/4c6ba7497e04ac3da65757091092a4a04960993525708cb15acfd577c321/regex-0.1.20120126.tar.gz#sha256=2c0da3b73812b7f0bdc140f257274be38e6bcd1fcc9b3f0109e656eb86dbb9a5 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120123.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 64.7 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/a7/c2/099974f0b3c2d91a30cf4056b315674cbfe378cb5b8e07d8e2dc83e7df15/regex-0.1.20120123.tar.gz#sha256=d760e8f5c285809a9838804c43f8f45a216fd15dc47941b47740376983fb5bb8 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120122.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 52.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/4b/b8/affbc348d5cc9d42fda80bda41ceb8d8223f86c0fa40c0ae66c9e27cd621/regex-0.1.20120122.tar.gz#sha256=3b483409ada69837d49bf22c470ed8d6de9da1fa0801418668cecd75a2805534 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120119.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 50.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/e1/39/a21a3bdebc4fb2d6448aa8dc8f740ed79b7bb445d209a5b824d3e272368e/regex-0.1.20120119.tar.gz#sha256=7aa10288041df6a6f24796f9b06b598d9ebe81cea5e6c3ee7cfaeddfdeb52b4b (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120115.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 39.5 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ef/ee/bad79c52e95baf22c553ac7a6eabb59426d0c1789900c580726e618bc1f4/regex-0.1.20120115.tar.gz#sha256=a306101e289c5b7820c1a05eb389228a88298e4e4dab43ba791579f4bf2c2952 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120114.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 52.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/21/2d/b7939aff10e9b46f06aab0b7de2b551440bc0107eef7ae03ab47c85fac0f/regex-0.1.20120114.tar.gz#sha256=0d1390e10afc2c0041443571f57d8d621024ce2bfb9891cb4178689c66c8b979 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120112.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 50.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/9d/7a/0553da46a1d937ad4f15b0158789f9dbbd5c020b787ad32802e1f15eaacf/regex-0.1.20120112.tar.gz#sha256=deee16ef8a5bfc7c7c63ca32489d5208d94715e665ee800d0df06416d27cfebc (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120105.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 51.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ac/a3/5fc5488c22ee31e3adcd866b3942cbf145cf3007fe54c9aee74b4ab1f03d/regex-0.1.20120105.tar.gz#sha256=90d069691e07e1e7c78a604ba926a7ad512ca381ede9636c26cf2432ed68e2a8 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20120103.tar.gz (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 53.3 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/f6/41/26c532a65e118e094ce215bd543c0889edcf6f35ce92704b11fbec1c1a9a/regex-0.1.20120103.tar.gz#sha256=d80aa62913e861f0942a22d2989b55cb4f74118549026890a6af2d7d607be24c (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20111223.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 25.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/0e/36/08bedf85cf145774a97d12a5f36646f26c4181c3b375f8e62347223aa503/regex-0.1.20111223.tar.gz#sha256=4bc255978d02e75db13bb146d976e885763ebe6cd84d74f95503526b5e6a1d1e (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20111103.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 51.2 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/db/46/f4178c7c622ed6ffd4870f2ba5653412ceac6e78989a9b0dba7feb06a315/regex-0.1.20111103.tar.gz#sha256=e44939f26bc30110d87561cfe7985e4221b5dbef0076b94463ef924dc295fc61 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20111014.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.3 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/6f/80/49c4d5416d3fca1a362485f28521a762eeaff4636276d4fcf34f8d5fd465/regex-0.1.20111014.tar.gz#sha256=51ce37359e51291c8e0551888e27ff1b902e76f98674b74c0aacd6f06c0b867d (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20111006.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.3 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/c1/e6/b43eda6080c1abf94f2078e473a8ba9d6a87495da7ba3d81d645470b67be/regex-0.1.20111006.tar.gz#sha256=cacfd83e56cb2c041b3ed0c7a153f6f7ae118106016360f08255e099a0a15c37 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20111005.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ea/9b/42c88e964535e406c0a63ec44ae51cd3c6f6797853904c0c4302af0eeb0a/regex-0.1.20111005.tar.gz#sha256=73b4bd044e5b904d09b92a8c78170b0d4eadb24188c6f9b85fb333561c6de8ee (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20111004.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 36.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/d5/44/67771c03725a780a913f464ddee981abd88538c2ec02ba9f4bd31c319472/regex-0.1.20111004.tar.gz#sha256=dde058f7ef37b64fe5001d0fc5040c3adb316fcd8bca3f60c4d568d95ff05def (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110929.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.2 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/f6/d2/bb6864e9f9c0b73c1f8ec4edf8c39e87123a359d9d5cfb4cd59843b61408/regex-0.1.20110929.tar.gz#sha256=1dc820b850dad9207c448836b05e6f2213370f005838cdf87f9ec5d7d152ee87 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110927.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 45.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/71/68/eaedee072de734939cf7e4f55fefa1547ff13322d533dad1a93e9a5b1f59/regex-0.1.20110927.tar.gz#sha256=85e3d4c97e91277ee5c977e1b1786e568e35cf3499808d53ae09821640af1467 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110922.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/70/74/0cab3e64d2a67dc8c9c2b7cb4e3f45950d48ee57782b7156239be4e4db3d/regex-0.1.20110922.tar.gz#sha256=83c802801be52fff7f66bd10be1259b2420547379627f37b215b09fd704a5252 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110917.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 35.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/82/77/b735b395489fed925bb4b18dbb5c5a0a5c5f5f144fab4a6ab140c51b03ad/regex-0.1.20110917.tar.gz#sha256=43e229a8b408154ad1f92aa65af45a1476b08e05c4775511b6ed86add90b48a4 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110717.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 65.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/69/ad/a828a1cb0e8880c77e0cfd8c694f7c162b008d44744e1bbd4fa71dc0702f/regex-0.1.20110717.tar.gz#sha256=32c28cfa3d09b0c1fcb3f0a6952297d654223e37b323d1af96154862cd1e0cba (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110702.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 62.2 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/47/91/bf26d84f64048b4d41edd9bbb62b98a983b71b34f42d8efb769a5e8f08f0/regex-0.1.20110702.tar.gz#sha256=8ce644ccbc28bcadc81b3f42edf87fdaf53e80837c96f1a9b3dd7371fe7e84df (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110627.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 48.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/d5/80/dee8aae0397de08aae9665152b42f5766bca8c4c51b426f4e7fae7a8190c/regex-0.1.20110627.tar.gz#sha256=978baf50dee32d5bbfcabca122d11a52a9b3edc9b0cfffa141433fe3003ce13f (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110623.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 61.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/f2/a8/87516fcd1cb3d856052c1c963bd0349f44031c7ae9eb01988044b28b12ab/regex-0.1.20110623.tar.gz#sha256=a51b98f48fd212520a47aa8c16862ef5a2b494e1b0ba2551e87e07db34b4ab43 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110616.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.5 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/a5/72/f46ac6ae1b13b0d06d59e7f9ffd934465674e67f27a5be1e47bd7abc06fc/regex-0.1.20110616.tar.gz#sha256=e54f7f3e024b3f65d585e5b9859656508d72f4e5623b7ada8c16e90afe340394 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110610.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/5a/74/1c2363aeba050a318ac3eb32e0e712cd8f1bba6d305f69eda4aaa254f402/regex-0.1.20110610.tar.gz#sha256=4b8b1876aef3ab899da9da057485b5c2afb193f1cf6b3d134a36b8e509da9092 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110609.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/41/a6/c0f637650455167034843433e2329295a24ec1376fdab5d4c138684f91ce/regex-0.1.20110609.tar.gz#sha256=a60b5415872199eb708dde1645242a60fd3265c63b4ad7b3df33d82838d7ef32 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110608.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 47.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/d0/ae/dcab795c5dd395e83664115119176ff90208ac1a639725d7e7237fcd4c65/regex-0.1.20110608.tar.gz#sha256=e9d4501cfa4f5da34432d3bbc99cb2ba48b39b3a6878b9841ded20d43ee7ccf4 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110524.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.0 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/98/4b/40f8368d2e6f3872e0ee3825d9ff0d632379c963ab5aadbf6c98dfb056c6/regex-0.1.20110524.tar.gz#sha256=ec9a79592fb768052e50a95ead91808172d55bec1a567b92c1f7467841b9f686 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110514.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 49.7 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/b2/23/bad44aaf12226f57f36fc9ae9c697580cc91a98719d7a22cb27b44fa3f39/regex-0.1.20110514.tar.gz#sha256=8d9b6a5374dd7a6acdf939b77b106b9973c02bc4a7e74d6db83c8b017840708a (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110510.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/47/60/fae6af300f2aec21a2835229e3d66d0aef3d71ffd9863847825ccf5320e9/regex-0.1.20110510.tar.gz#sha256=b9cf646e0e61c37b9e5305150dcf85802890cff723a038ef162ea100d76d56b7 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110504.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/16/c6/5574ccb68a7d84a1534e3cb6469a2712750187a58d15dfd772f698e5a3c3/regex-0.1.20110504.tar.gz#sha256=46170d1a6570e8f52c83256d752e0974e3385f18ccde592ee755eda8d5a28673 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110502.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.5 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/07/53/a5d154a1825c0bcda78e576a14e6eaecb4adf0b292bab949a77942ff4be9/regex-0.1.20110502.tar.gz#sha256=4c3f6d9909181a801cd1000e1e9e2382f328f3e655f0fb16f248ca855fcc8deb (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110429.tar.gz (954 kB)\n",
            "\u001b[K     |████████████████████████████████| 954 kB 54.0 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/6a/ad/1baff7bbc2072292ec8165daa187f8ec9bafe7848111f280486297aa95d8/regex-0.1.20110429.tar.gz#sha256=7e60de2ad0dec7dc2940750a1d549af2f1d0514496607c27b1b3b4826a480a22 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110315.tar.gz (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 46.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/84/03/9a0b4293e6e92173386ef733e6aa1e3af500b1718247353cfd5b857c2eee/regex-0.1.20110315.tar.gz#sha256=3c050a63a616a9ae3e9c75bd8093e66d773696bccbf87394957d0b53d14aed92 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110314.tar.gz (945 kB)\n",
            "\u001b[K     |████████████████████████████████| 945 kB 53.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/87/bc/8ef23817e442a232fdbe322db2601d3f71d95f40082e88d994a8e8ee699d/regex-0.1.20110314.tar.gz#sha256=fe3c075e4ac6411f78afe76d270a4edb0b066ce4f096d14b7b24e92f8a08aeb2 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110313.tar.gz (945 kB)\n",
            "\u001b[K     |████████████████████████████████| 945 kB 51.7 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/02/48/c1134ce0de39f035fb9eabefbc1b8874dea085b443969107877e49fb3640/regex-0.1.20110313.tar.gz#sha256=295b1cc0be5a7763ee8f2340d028deff4f42d4bc75d752ad6306d926b9c66237 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110124.tar.gz (941 kB)\n",
            "\u001b[K     |████████████████████████████████| 941 kB 53.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/55/e9/54bbeac5d9c3fedd8055611da568fbef10d51e7a0726b9e3d3bca4ea7bf3/regex-0.1.20110124.tar.gz#sha256=cdf94e845db96f358bb31aac6ff0035ae0406c74c028b981271e1bada31c6e86 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110106.tar.gz (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 55.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/2f/dd/00fe6e0c002c92158877f4d34c4bf2668f831c73f7a0f5c58c0a2bafec18/regex-0.1.20110106.tar.gz#sha256=f48448bf8d53cffd19aaef952c46294ccce529aebe68af0ce6cb835fcfb9e7f1 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20110104.tar.gz (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 64.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/c8/fe/199f72e35793350e48e2970088767b9714938201878980c5de4aa455fca7/regex-0.1.20110104.tar.gz#sha256=2532d4c30b6b0ca477747b2aca7b3da1bd5ed180a60e74174b7f52fd9b59c8a9 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101231.tar.gz (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 55.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/4a/c1/78d4cdace7fd9c6c17691453aafb9ab07328e0421df1b738cc3c6be8d82d/regex-0.1.20101231.tar.gz#sha256=4469ab16b8a28974c7760b22ce4f8a8c45fb2fe868bc79029ead20531668dd11 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101230.tar.gz (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 52.2 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/d2/3a/dd35a53e897a19ba67a1210d7bd781fdeedc44d1144d754454ed6d739fa1/regex-0.1.20101230.tar.gz#sha256=d217a91a7e7e3d89a26b7d9a7f4566fe21da646094a8fe63e341393c933007a2 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101229.tar.gz (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 52.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/02/13/2e89d0d314a8a101bbd98483c7a27abb89328d2701b456265c23b7054b5b/regex-0.1.20101229.tar.gz#sha256=669de970b530e6ad95afab368c591f17e84cca922ad4a0facaec36259bfbcad0 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101228.tar.gz (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 50.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/28/a5/1d6408fb1d889503ca1162ab77c2eb0c98d314eb356ab5839355d76fc33a/regex-0.1.20101228.tar.gz#sha256=f1286f8a212d35093cc2faf21f34da3996b05c02cc75f1141065c93dfef98ab2 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101224.tar.gz (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 63.7 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/b1/de/b347ebc0900572e3b11dc9a96c8ebbefa7b268ee20c967e87d8932d62232/regex-0.1.20101224.tar.gz#sha256=cd64cc5699242833ead3cba88faf3d057c8839f5a78979cef0177f4422f3951b (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101210.tar.gz (953 kB)\n",
            "\u001b[K     |████████████████████████████████| 953 kB 57.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/57/b8/12207617512bc84a6e8c0e786eef1f041260ec8024ae88262395acc643ec/regex-0.1.20101210.tar.gz#sha256=766d6833dc8a78e582203ec2a9e30d698c3a574252cc445570c7ead97f415c0b (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101207.tar.gz (810 kB)\n",
            "\u001b[K     |████████████████████████████████| 810 kB 47.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/0d/33/b151f7797c5956ab9d8b08fd21bd11e0484c48a2ef749198e659836dc232/regex-0.1.20101207.tar.gz#sha256=db7f13c9d8b90a59d4b56164ec1a29266273b7bdb454f1524e210460f0d57fff (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101130.tar.gz (801 kB)\n",
            "\u001b[K     |████████████████████████████████| 801 kB 52.5 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/87/ae/1bb097ab063302517a4274c2f2207c74beb36ac5d5d6a1acc8c50f6783a9/regex-0.1.20101130.tar.gz#sha256=7ab9f5a80d9f05e83ff33c8b28921ab740a03743241b06eaec13c373ffc1789f (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101123.tar.gz (800 kB)\n",
            "\u001b[K     |████████████████████████████████| 800 kB 54.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/e3/35/0c2793926ac0f1b5ccc04fe5fd7dd2ce66bd72e1ff8abd11785d8357b317/regex-0.1.20101123.tar.gz#sha256=f6feefb1a4e0573d4eccc9be0eb1384fac7e93db98b700da3e66c6d0a6eeb921 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101121.tar.gz (800 kB)\n",
            "\u001b[K     |████████████████████████████████| 800 kB 22.2 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/a8/d6/5de004412963416464ac4b46fbab4934e624de305d88b5b83696ec96b314/regex-0.1.20101121.tar.gz#sha256=12e76c1a3f55ceeab8ccea22450ac26c48c9d81ec49fbd605f73847468d73361 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101120.tar.gz (796 kB)\n",
            "\u001b[K     |████████████████████████████████| 796 kB 62.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/93/fe/1265d352e90fb59cf7aefb3b85995a0735a4b277a1f2483cf1d4b4ea9336/regex-0.1.20101120.tar.gz#sha256=f8fa1ee871c514017288d03530002148cafd10c038d5c1acd8dbb584a7ddcdd6 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101113.tar.gz (785 kB)\n",
            "\u001b[K     |████████████████████████████████| 785 kB 62.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/94/39/3ed8320a3cf71863b0012b4c24e5dabd5a64d4c8f47a832c39b957acde5b/regex-0.1.20101113.tar.gz#sha256=eab12c322a47bb4d7c594eff9499dd2c88f79fa8f370c9adad6015cefd2bd035 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101106.tar.gz (782 kB)\n",
            "\u001b[K     |████████████████████████████████| 782 kB 43.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/6a/76/19cf1fcf793b9ac480dcd4e9260499be2811f67eea5b55f8150fa727b619/regex-0.1.20101106.tar.gz#sha256=c3ccff7dd6cee8bf7dc1f3794c8dfec2b47115d7245e035d6ae6f8afa5e0e45f (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101102.tar.gz (782 kB)\n",
            "\u001b[K     |████████████████████████████████| 782 kB 23.0 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/f6/c1/f15b16d12a4732059203af559c15ebdd1474344f15703e1d37e7ecd176e6/regex-0.1.20101102.tar.gz#sha256=9c0ec2602a068394c260b87710387dd050b435c4795ce09445b335768084b32a (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101101.tar.gz (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 64.5 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/c1/ac/0bd366c48e0449b1ea630a2b8be2c4ad7ebd024895791889a8870c9c7283/regex-0.1.20101101.tar.gz#sha256=c639b4e30b11bcd1366a9e7ef7153fa84d24c5c670f31c444eebfdccef6008b8 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101030.tar.gz (775 kB)\n",
            "\u001b[K     |████████████████████████████████| 775 kB 47.0 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/f7/6c/1514497b6b192428a8da675515289d1a85c4ec3b2679d80acf136943424e/regex-0.1.20101030.tar.gz#sha256=ef2df5f3c7d2decfcb246e19861291f4323978546aed60d3b65debd771e417b0 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101029.tar.gz (775 kB)\n",
            "\u001b[K     |████████████████████████████████| 775 kB 43.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/87/fe/9fe474d9d695ebed215679123b53a4c3b327d9356aff7d7e2fd7a124f495/regex-0.1.20101029.tar.gz#sha256=4ab63b1d82075901b61d4b500b86c3bb22fd889215d94232a3a779eb2144313e (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20101009.tar.gz (775 kB)\n",
            "\u001b[K     |████████████████████████████████| 775 kB 66.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/5d/f4/34bbc1b98d453a51e8dff3ce019e77e57ce3452b5bda94bee04115dbab51/regex-0.1.20101009.tar.gz#sha256=ecd10139f194fde912eaa79df996b99675b2824b2707d6c4e3ee8a5b0a8ba565 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100918.tar.gz (774 kB)\n",
            "\u001b[K     |████████████████████████████████| 774 kB 44.7 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/0f/f1/06dd1dd8a80b095f39f463d9b4dad684be7ff50bc224bd4b9db13fb95872/regex-0.1.20100918.tar.gz#sha256=8247eb12b7a32f1e56380cb33bc706a875ecb9b3020ac4d04a238c4bcf1777db (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100913.tar.gz (777 kB)\n",
            "\u001b[K     |████████████████████████████████| 777 kB 47.5 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/3b/00/c2d9c2605fd81b44eb74e2c858bada97a4d529b2b2503736283c2454d456/regex-0.1.20100913.tar.gz#sha256=16d622a8c40c671d6e1075c606cf5f098b15bd74f032766e85fac4e130416e8e (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100912.tar.gz (774 kB)\n",
            "\u001b[K     |████████████████████████████████| 774 kB 53.3 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/99/ea/c751937f8a3a4c1bfb6d2fa38649856dd0aeb4c104f9d3a3935f7d380878/regex-0.1.20100912.tar.gz#sha256=44f5e1fa61ed57e217ea550110c30b0121ed42f00b6ae77ee602a10a1d956051 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100824.tar.gz (791 kB)\n",
            "\u001b[K     |████████████████████████████████| 791 kB 33.2 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/c2/69/cedd5a72a43461aca1088609f2c67b2f318040120d7d180e4ecab02c6a72/regex-0.1.20100824.tar.gz#sha256=92de0e42591b46aae13b883c7ce3e7fa845a6b735b016d294d4e3b5663edd0ee (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100816.tar.gz (787 kB)\n",
            "\u001b[K     |████████████████████████████████| 787 kB 64.0 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ad/2e/5843e32043f095f29e5f8a93151086a69d37a26526f2aca00365cbc7908b/regex-0.1.20100816.tar.gz#sha256=dc699066c126551b5f3fda0067ac8db1494dca948344cce03a7bbf3c2ee6ef06 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100814.tar.gz (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 57.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/47/e6/63a7fac8d1ccead37ff67ea23814d757fe2a73ebca1b3038c4cd26733e39/regex-0.1.20100814.tar.gz#sha256=e1eb17926dc11a33f6059505b22564779c4af71a21f6e6389d6b76fbd08a98d3 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100725.tar.gz (711 kB)\n",
            "\u001b[K     |████████████████████████████████| 711 kB 63.5 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/4b/e5/d543b1b1b42637e69e69341fbfe873817d49b03b44e761aff750e79d777a/regex-0.1.20100725.tar.gz#sha256=f2b3c52167cb57333305d08f3eb5a596e9cfabc7cd86dbecb0c1b8106745ef9f (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100719.tar.gz (701 kB)\n",
            "\u001b[K     |████████████████████████████████| 701 kB 65.3 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/3e/0d/0a0868a05eb86415358d61bc89880cc817f7a59eee7de69b12d61eaf2690/regex-0.1.20100719.tar.gz#sha256=6b80499e62b9911161dff0a97d8c1d2d890d52089a92a8d027b993c546330148 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100709.1.tar.gz (680 kB)\n",
            "\u001b[K     |████████████████████████████████| 680 kB 39.2 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/c2/8e/a982d18e97e0285a25b6d0844a59eef129506c5e9708a9c7b76601527f6b/regex-0.1.20100709.1.tar.gz#sha256=6322cc9d6a0e84de390384046821bb61b63599aab22191da018f1610b20584ad (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100709.tar.gz (755 kB)\n",
            "\u001b[K     |████████████████████████████████| 755 kB 32.7 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/74/6d/921e18701796e62d59e29ecddd6890382d0cde3553ed8c93b7366b27aedb/regex-0.1.20100709.tar.gz#sha256=78f470fd0b9448ca2cee933e6e62a15ce7f4574cd54f6245e66b1f10917dd396 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100706.1.tar.gz (751 kB)\n",
            "\u001b[K     |████████████████████████████████| 751 kB 54.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/5f/f0/f61efb5f847ab96780561f949930987c14d0ed04cd30ed7d2201d441729f/regex-0.1.20100706.1.tar.gz#sha256=74ad571f05a8ac1fa7a2599d0eb84c68e0c254c337b0fda2ad83beb9413eec0e (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100706.tar.gz (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 45.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/61/67/05c631e4cbb9bc23200c8cdd3781990fb8c6ade59cf889042497a7c687a6/regex-0.1.20100706.tar.gz#sha256=25e7568cdbdd12505309dafd5662cb0ec3f33301d321cd55a19603da863fb00b (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100331.tar.gz (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 37.3 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/13/03/d9acbd1a9a4905c5ede43d4a85f345e029471bc045474968c012b5b5a6c8/regex-0.1.20100331.tar.gz#sha256=88fa3f89b93577130dc205a658be3ffe69a0104f147fd2b575f2a09d6a8aef21 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100323.tar.gz (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 54.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/e3/24/2e09238c5d907b32fa32e6551bf29aa3e7e06280e8f2504f042fda49d3f3/regex-0.1.20100323.tar.gz#sha256=07f2de51f70bdf313e445caa932e5caf7b020fb3821ea6d56f65eecb7ee32955 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100305.tar.gz (337 kB)\n",
            "\u001b[K     |████████████████████████████████| 337 kB 43.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/5b/00/8d5de4d1f987970e9baf00a28282a2b3a160a7e0d5ae02507057d1594eba/regex-0.1.20100305.tar.gz#sha256=4044a79f88b732d8803781ba9f3320e89d7145639223c1f1f5dac00e58b5f869 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100226.tar.gz (301 kB)\n",
            "\u001b[K     |████████████████████████████████| 301 kB 53.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ba/09/c7f091db6434c7cdaf3fbf7ed0eae6b07f9fe570aabbcfdcf91e90b829ae/regex-0.1.20100226.tar.gz#sha256=9c1e63788f9bd92150cfcbfbf7d62c8b862e6a428a5ab8cf19d9418761caffd6 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading regex-0.1.20100217.tar.gz (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 48.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/9d/13/4a931402b23f8134415669585f9dab54b3669c84684db1ec409a706d70b9/regex-0.1.20100217.tar.gz#sha256=c02dfe1ffc9fce1d9bd1392723a3e7b0eebce67fdfa80511ff8a631ffa73afc2 (from https://pypi.org/simple/regex/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pytz to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytz>=2017.2\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 57.0 MB/s \n",
            "\u001b[?25h  Downloading pytz-2020.5-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 49.9 MB/s \n",
            "\u001b[?25h  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 68.9 MB/s \n",
            "\u001b[?25h  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 70.9 MB/s \n",
            "\u001b[?25h  Downloading pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 58.5 MB/s \n",
            "\u001b[?25h  Downloading pytz-2019.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[K     |████████████████████████████████| 508 kB 71.3 MB/s \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRFul7zcnLJ9"
      },
      "source": [
        "import sys\n",
        "sys.argv=['']\n",
        "del sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "fKPbXAIjNvcF",
        "outputId": "6d9d34aa-0848-4885-b641-709d8c196904"
      },
      "source": [
        "#transfer.py\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "from bert_pretrained import bert_tokenizer\n",
        "from options import args\n",
        "\n",
        "\n",
        "def style_transfer(encoder=None, generator=None, text_path=None, n_samples=100):\n",
        "    # save result if path is given\n",
        "    if args.transfer_result_save_path is not None:\n",
        "        fw = open(args.transfer_result_save_path, 'a')\n",
        "    else:\n",
        "        fw = None\n",
        "\n",
        "    # interactive mode\n",
        "    if text_path is None:\n",
        "        if fw is not None:\n",
        "            fw.write('\\n' + \"=\" * 50 + '\\n')\n",
        "            fw.write(\"Interactive transfer from {} -> {}\\n\".format(\n",
        "                str(1 - args.transfer_to),\n",
        "                str(args.transfer_to)\n",
        "            ))\n",
        "            fw.write(\"=\" * 50 + '\\n')\n",
        "        try:\n",
        "            while True:\n",
        "                fmt = \"Enter text to transfer to style {} (Ctrl+C to exit): \"\n",
        "                text = input(fmt.format(args.transfer_to))\n",
        "                tokens = bert_tokenizer.encode(text, add_special_tokens=False)\n",
        "                tokens = (\n",
        "                    [bert_tokenizer.bos_token_id]\n",
        "                    + tokens\n",
        "                    + [bert_tokenizer.eos_token_id]\n",
        "                )\n",
        "                tokens = torch.LongTensor([tokens]).transpose(0, 1)\n",
        "                original_label = torch.FloatTensor([1 - args.transfer_to])\n",
        "                output = generate_text(\n",
        "                    encoder.to(args.device),\n",
        "                    generator.to(args.device),\n",
        "                    original_label.to(args.device),\n",
        "                    tokens.to(args.device)\n",
        "                )\n",
        "                print(\"Transfer result:\", output)\n",
        "                if fw is not None:\n",
        "                    fw.write(text + ' -> ' + output + '\\n')\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            if fw is not None:\n",
        "                fw.close()\n",
        "            print(\"\\nEnd interactive transfer\\n\")\n",
        "\n",
        "    # load data from text path\n",
        "    else:\n",
        "        if fw is not None:\n",
        "            fw.write('\\n' + \"=\" * 50 + '\\n')\n",
        "            fw.write(\"Transfer from file: {}\\n\".format(text_path))\n",
        "            fw.write(\"Number of samples: {}\\n\".format(n_samples))\n",
        "            fw.write(\"=\" * 50 + '\\n')\n",
        "\n",
        "        pbar = tqdm(total=n_samples)\n",
        "        counter = 0\n",
        "        inputs0, inputs1 = [], []\n",
        "        outputs0, outputs1 = [], []\n",
        "        with open(text_path, 'r') as text_file:\n",
        "            for line in text_file:\n",
        "                counter += 1\n",
        "                if counter == 1:\n",
        "                    continue\n",
        "                _, text, label = line.strip().split('\\t')\n",
        "                tokens = bert_tokenizer.encode(text, add_special_tokens=False)\n",
        "                tokens = (\n",
        "                    [bert_tokenizer.bos_token_id]\n",
        "                    + tokens\n",
        "                    + [bert_tokenizer.eos_token_id]\n",
        "                )\n",
        "                tokens = torch.LongTensor([tokens]).transpose(0, 1)\n",
        "                original_label = torch.FloatTensor([int(label)])\n",
        "                output = generate_text(\n",
        "                    encoder.to(args.device),\n",
        "                    generator.to(args.device),\n",
        "                    original_label.to(args.device),\n",
        "                    tokens.to(args.device)\n",
        "                )\n",
        "                if int(label) == 0:\n",
        "                    inputs0.append(text)\n",
        "                    outputs0.append(output)\n",
        "                else:\n",
        "                    inputs1.append(text)\n",
        "                    outputs1.append(output)\n",
        "                pbar.update()\n",
        "                if fw is not None:\n",
        "                    fw.write(label + ' > ' + str(1-int(label)) + ': '+ text + ' -> ' + output + '\\n')\n",
        "                if counter > n_samples:\n",
        "                    break\n",
        "\n",
        "        if fw is not None:\n",
        "            fw.close()\n",
        "        return inputs0, inputs1, outputs0, outputs1\n",
        "\n",
        "\n",
        "def generate_text(encoder, generator, original_label, tokens):\n",
        "    src_len = [len(tokens)]\n",
        "    predictions = generator.transfer(\n",
        "        encoder(original_label, tokens, src_len),  # hidden state\n",
        "        1 - original_label,  # transfer label\n",
        "        eos_token_id=bert_tokenizer.eos_token_id,\n",
        "        max_len=args.transfer_max_len\n",
        "    )\n",
        "    # change this part to first occurence of ber_tokenizer.eos_token_id\n",
        "    #if predictions[-1] == bert_tokenizer.eos_token_id:\n",
        "    #    predictions = predictions[:-1]\n",
        "\n",
        "    try:\n",
        "        eos_idx = predictions.index(bert_tokenizer.eos_token_id)\n",
        "        predictions = predictions[:eos_idx]\n",
        "    except ValueError:\n",
        "        predictions=predictions[:-1]\n",
        "    return bert_tokenizer.decode(predictions)\n",
        "\n",
        "\n",
        "def _transfer():\n",
        "    #cuda device 설정\n",
        "    device = torch.device('cuda:{}'.format(args.cuda_device) if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # 1. get model\n",
        "    embedding = get_bert_word_embedding().to(device).eval()\n",
        "    encoder = Encoder(embedding, args.dim_y, args.dim_z).to(device).eval()\n",
        "    generator = Generator(embedding, args.dim_y, args.dim_z, args.temperature, bert_tokenizer.bos_token_id, use_gumbel=args.use_gumbel).to(device).eval()\n",
        "    \n",
        "    # 2. load checkpoint\n",
        "    ckpt = torch.load(args.ckpt_path, map_location=device)\n",
        "    embedding.load_state_dict(ckpt['embedding_state_dict'])\n",
        "    encoder.load_state_dict(ckpt['encoder_state_dict'])\n",
        "    generator.load_state_dict(ckpt['generator_state_dict'])\n",
        "    \n",
        "    # 3. transfer!\n",
        "    if args.transfer_result_save_path is not None:\n",
        "        fw = open(args.transfer_result_save_path, 'w')\n",
        "    else:\n",
        "        fw = None\n",
        "            \n",
        "    if args.test_text_path is None:\n",
        "        # interactive mode\n",
        "        while True:\n",
        "            text = input(\"Enter text to transfer to stye {} (Ctrl+C to exit): \".format(args.transfer_to))\n",
        "            text_tokens = [bert_tokenizer.bos_token_id] + bert_tokenizer.encode(text, add_special_tokens=False) + [bert_tokenizer.eos_token_id]\n",
        "            #텍스트 토큰 만들기 bos+encoding+eos\n",
        "            text_tokens_tensor = torch.LongTensor([text_tokens]).transpose(0, 1).to(device)\n",
        "            #long tensor transpose\n",
        "            src_len = [len(text_tokens)]\n",
        "            original_label = torch.FloatTensor([1-args.transfer_to]).to(device)\n",
        "            transfer_label = torch.FloatTensor([args.transfer_to]).to(device)\n",
        "            \n",
        "            z = encoder(original_label, text_tokens_tensor, src_len)\n",
        "            predictions = generator.transfer(z, transfer_label, eos_token_id=bert_tokenizer.eos_token_id, max_len=args.transfer_max_len)\n",
        "            if predictions[-1] == bert_tokenizer.eos_token_id:\n",
        "                predictions = predictions[:-1]\n",
        "                \n",
        "            result = bert_tokenizer.decode(predictions)\n",
        "            print(\"Transfer Result:\", result)\n",
        "            if fw is not None:\n",
        "                fw.write(text + ' -> ' + result + '\\n')\n",
        "                \n",
        "            if args.test_recon:\n",
        "                recon = generator.transfer(z, original_label, eos_token_id=bert_tokenizer.eos_token_id, max_len=args.transfer_max_len)\n",
        "                if recon[-1] == bert_tokenizer.eos_token_id:\n",
        "                    recon = recon[:-1]\n",
        "                print(\"Recon:\", bert_tokenizer.decode(recon))\n",
        "            \n",
        "    else:\n",
        "\n",
        "        for line in args.test_text_path:\n",
        "            line = line.strip()\n",
        "            text = line\n",
        "            text_tokens = [bert_tokenizer.bos_token_id] + bert_tokenizer.encode(text, add_special_tokens=False) + [bert_tokenizer.eos_token_id]\n",
        "            text_tokens_tensor = torch.LongTensor([text_tokens]).transpose(0, 1).to(device)\n",
        "            src_len = [len(text_tokens)]\n",
        "            original_label = torch.FloatTensor([1-args.transfer_to]).to(device)\n",
        "            transfer_label = torch.FloatTensor([args.transfer_to]).to(device)\n",
        "            \n",
        "            z = encoder(original_label, text_tokens_tensor, src_len)\n",
        "            predictions = generator.transfer(z, transfer_label, eos_token_id=bert_tokenizer.eos_token_id, max_len=args.transfer_max_len)\n",
        "            if predictions[-1] == bert_tokenizer.eos_token_id:\n",
        "                predictions = predictions[:-1]\n",
        "                \n",
        "            result = bert_tokenizer.decode(predictions)\n",
        "            print(\"Transfer Result:\", result)\n",
        "            if fw is not None:\n",
        "                fw.write(text + ' -> ' + result + '\\n')\n",
        "                \n",
        "            if args.test_recon:\n",
        "                recon = generator.transfer(z, original_label, eos_token_id=bert_tokenizer.eos_token_id, max_len=args.transfer_max_len)\n",
        "                if recon[-1] == bert_tokenizer.eos_token_id:\n",
        "                    recon = recon[:-1]\n",
        "                print(\"Recon:\", bert_tokenizer.decode(recon))\n",
        "            \n",
        "if __name__ == '__main__':\n",
        "    transfer()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage:  [-h] --ckpt_path CKPT_PATH [--clf_ckpt_path CLF_CKPT_PATH]\n",
            "        [--dataset {yelp,nsmc}] [--text_file_path TEXT_FILE_PATH]\n",
            "        [--val_text_file_path VAL_TEXT_FILE_PATH] [--batch_size BATCH_SIZE]\n",
            "        [--max_seq_length MAX_SEQ_LENGTH] [--num_workers NUM_WORKERS]\n",
            "        [--dim_y DIM_Y] [--dim_z DIM_Z] [--dim_emb DIM_EMB]\n",
            "        [--filter_sizes FILTER_SIZES [FILTER_SIZES ...]]\n",
            "        [--n_filters N_FILTERS] [--epochs EPOCHS]\n",
            "        [--weight_decay WEIGHT_DECAY] [--max_grad_norm MAX_GRAD_NORM]\n",
            "        [--lr LR] [--disc_lr DISC_LR] [--temperature TEMPERATURE]\n",
            "        [--use_gumbel USE_GUMBEL] [--rho RHO] [--two_stage TWO_STAGE]\n",
            "        [--second_stage_num SECOND_STAGE_NUM]\n",
            "        [--gan_type {vanilla,wgan-gp,lsgan}] [--gp_weight GP_WEIGHT]\n",
            "        [--log_interval LOG_INTERVAL] [--language {ko,en}]\n",
            "        [--threshold THRESHOLD] [--mode {train,transfer,interactive}]\n",
            "        [--test_text_path TEST_TEXT_PATH] [--transfer_to {0,1}]\n",
            "        [--n_samples N_SAMPLES] [--transfer_max_len TRANSFER_MAX_LEN]\n",
            "        [--transfer_result_save_path TRANSFER_RESULT_SAVE_PATH]\n",
            "        [--cuda_device CUDA_DEVICE] [--load_ckpt LOAD_CKPT]\n",
            ": error: the following arguments are required: --ckpt_path\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad5JsMDkOELh"
      },
      "source": [
        "#train.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "import time\n",
        "import itertools\n",
        "import sys, os\n",
        "\n",
        "from dataloader import get_dataloader_for_style_transfer\n",
        "from model import Encoder, Generator, Discriminator\n",
        "from bert_pretrained import bert_tokenizer, get_bert_word_embedding, FILE_ID\n",
        "from bert_pretrained.classifier import BertClassifier\n",
        "from loss import loss_fn, gradient_penalty\n",
        "from evaluate import calculate_accuracy, calculate_frechet_distance\n",
        "from transfer import style_transfer\n",
        "\n",
        "from options import args\n",
        "from utils import AverageMeter, ProgressMeter, download_google, Metric_Printer\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        # get models\n",
        "        embedding = get_bert_word_embedding()\n",
        "        if os.path.isfile(args.load_ckpt):\n",
        "            self.models = torch.load(args.load_ckpt)\n",
        "        else:\n",
        "            self.models = nn.ModuleDict({\n",
        "                'embedding': embedding,\n",
        "                'encoder': Encoder(embedding, args.dim_y, args.dim_z),\n",
        "                'generator': Generator(\n",
        "                    embedding, args.dim_y, args.dim_z, args.temperature,\n",
        "                    bert_tokenizer.bos_token_id, use_gumbel=args.use_gumbel\n",
        "                ),\n",
        "                'disc_0': Discriminator(  # 0: real, 1: fake\n",
        "                    args.dim_y + args.dim_z, args.n_filters, args.filter_sizes\n",
        "                ),\n",
        "                'disc_1': Discriminator(  # 1: real, 0: fake\n",
        "                    args.dim_y + args.dim_z, args.n_filters, args.filter_sizes\n",
        "                ),\n",
        "            })\n",
        "        self.models.to(args.device)\n",
        "        # pretrained classifier\n",
        "        self.clf = BertClassifier()\n",
        "        if args.clf_ckpt_path is not None:\n",
        "            download_google(FILE_ID, args.clf_ckpt_path)\n",
        "            ckpt = torch.load(\n",
        "                args.clf_ckpt_path,\n",
        "                map_location=lambda storage, loc: storage\n",
        "            )\n",
        "            self.clf.load_state_dict(ckpt['model_state_dict'])\n",
        "        self.clf.to(args.device)\n",
        "        self.clf.eval()\n",
        "\n",
        "        # get dataloaders\n",
        "        self.train_loaders = get_dataloader_for_style_transfer(\n",
        "            args.text_file_path, shuffle=True, drop_last=True\n",
        "        )\n",
        "        # label placeholders\n",
        "        self.zeros = torch.zeros(args.batch_size, 1).to(args.device)\n",
        "        self.ones = torch.ones(args.batch_size, 1).to(args.device)\n",
        "\n",
        "        # get optimizers\n",
        "        self.optimizer = optim.AdamW(\n",
        "            list(itertools.chain.from_iterable([\n",
        "                list(self.models[k].parameters())\n",
        "                for k in ['embedding', 'encoder', 'generator']\n",
        "            ])),\n",
        "            lr=args.lr,\n",
        "            betas=(0.5, 0.9),\n",
        "            weight_decay=args.weight_decay\n",
        "        )\n",
        "        self.disc_optimizer = optim.AdamW(\n",
        "            list(itertools.chain.from_iterable([\n",
        "                list(self.models[k].parameters())\n",
        "                for k in ['disc_0', 'disc_1']\n",
        "            ])),\n",
        "            lr=args.disc_lr,\n",
        "            betas=(0.5, 0.9),\n",
        "            weight_decay=args.weight_decay\n",
        "        )\n",
        "\n",
        "        self.epoch = 0\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.models.train()\n",
        "        self.epoch += 1\n",
        "\n",
        "        # record training statistics\n",
        "        avg_meters = {\n",
        "            'loss_rec': AverageMeter('Loss Rec', ':.4e'),\n",
        "            'loss_adv': AverageMeter('Loss Adv', ':.4e'),\n",
        "            'loss_disc': AverageMeter('Loss Disc', ':.4e'),\n",
        "            'time': AverageMeter('Time', ':6.3f')\n",
        "        }\n",
        "        progress_meter = ProgressMeter(\n",
        "            len(self.train_loaders[0]),\n",
        "            avg_meters.values(),\n",
        "            prefix=\"Epoch: [{}]\".format(self.epoch)\n",
        "        )\n",
        "\n",
        "        # begin training from minibatches\n",
        "        for ix, (data_0, data_1) in enumerate(zip(*self.train_loaders)):\n",
        "            start_time = time.time()\n",
        "\n",
        "            # load text and labels\n",
        "            src_0, src_len_0, labels_0 = data_0\n",
        "            src_0, labels_0 = src_0.to(args.device), labels_0.to(args.device)\n",
        "            src_1, src_len_1, labels_1 = data_1\n",
        "            src_1, labels_1 = src_1.to(args.device), labels_1.to(args.device)\n",
        "\n",
        "            # encode\n",
        "            encoder = self.models['encoder']\n",
        "            z_0 = encoder(labels_0, src_0, src_len_0)  # (batch_size, dim_z)\n",
        "            z_1 = encoder(labels_1, src_1, src_len_1)\n",
        "\n",
        "            # recon & transfer\n",
        "            generator = self.models['generator']\n",
        "            inputs_0 = (z_0, labels_0, src_0)\n",
        "            h_ori_seq_0, pred_ori_0 = generator(*inputs_0, src_len_0, False)\n",
        "            h_trans_seq_0_to_1, _ = generator(*inputs_0, src_len_1, True)\n",
        "\n",
        "            inputs_1 = (z_1, labels_1, src_1)\n",
        "            h_ori_seq_1, pred_ori_1 = generator(*inputs_1, src_len_1, False)\n",
        "            h_trans_seq_1_to_0, _ = generator(*inputs_1, src_len_0, True)\n",
        "\n",
        "            # discriminate real and transfer\n",
        "            disc_0, disc_1 = self.models['disc_0'], self.models['disc_1']\n",
        "            d_0_real = disc_0(h_ori_seq_0.detach())  # detached\n",
        "            d_0_fake = disc_0(h_trans_seq_1_to_0.detach())\n",
        "            d_1_real = disc_1(h_ori_seq_1.detach())\n",
        "            d_1_fake = disc_1(h_trans_seq_0_to_1.detach())\n",
        "\n",
        "            # discriminator loss\n",
        "            loss_disc = (\n",
        "                loss_fn(args.gan_type)(d_0_real, self.ones)\n",
        "                + loss_fn(args.gan_type)(d_0_fake, self.zeros)\n",
        "                + loss_fn(args.gan_type)(d_1_real, self.ones)\n",
        "                + loss_fn(args.gan_type)(d_1_fake, self.zeros)\n",
        "            )\n",
        "            # gradient penalty\n",
        "            if args.gan_type == 'wgan-gp':\n",
        "                loss_disc += args.gp_weight * gradient_penalty(\n",
        "                    h_ori_seq_0,            # real data for 0\n",
        "                    h_trans_seq_1_to_0,     # fake data for 0\n",
        "                    disc_0\n",
        "                )\n",
        "                loss_disc += args.gp_weight * gradient_penalty(\n",
        "                    h_ori_seq_1,            # real data for 1\n",
        "                    h_trans_seq_0_to_1,     # fake data for 1\n",
        "                    disc_1\n",
        "                )\n",
        "            avg_meters['loss_disc'].update(loss_disc.item(), src_0.size(0))\n",
        "\n",
        "            self.disc_optimizer.zero_grad()\n",
        "            loss_disc.backward()\n",
        "            self.disc_optimizer.step()\n",
        "\n",
        "            # reconstruction loss\n",
        "            loss_rec = (\n",
        "                F.cross_entropy(    # Recon 0 -> 0\n",
        "                    pred_ori_0.view(-1, pred_ori_0.size(-1)),\n",
        "                    src_0[1:].view(-1),\n",
        "                    ignore_index=bert_tokenizer.pad_token_id,\n",
        "                    reduction='sum'\n",
        "                )\n",
        "                + F.cross_entropy(  # Recon 1 -> 1\n",
        "                    pred_ori_1.view(-1, pred_ori_1.size(-1)),\n",
        "                    src_1[1:].view(-1),\n",
        "                    ignore_index=bert_tokenizer.pad_token_id,\n",
        "                    reduction='sum'\n",
        "                )\n",
        "            ) / (2.0 * args.batch_size)  # match scale with the orginal paper\n",
        "            avg_meters['loss_rec'].update(loss_rec.item(), src_0.size(0))\n",
        "\n",
        "            # generator loss\n",
        "            d_0_fake = disc_0(h_trans_seq_1_to_0)  # not detached\n",
        "            d_1_fake = disc_1(h_trans_seq_0_to_1)\n",
        "            loss_adv = (\n",
        "                loss_fn(args.gan_type, disc=False)(d_0_fake, self.ones)\n",
        "                + loss_fn(args.gan_type, disc=False)(d_1_fake, self.ones)\n",
        "            ) / 2.0  # match scale with the original paper\n",
        "            avg_meters['loss_adv'].update(loss_adv.item(), src_0.size(0))\n",
        "\n",
        "            # XXX: threshold for training stability\n",
        "            if (not args.two_stage):\n",
        "                if (args.threshold is not None\n",
        "                        and loss_disc < args.threshold):\n",
        "                    loss = loss_rec + args.rho * loss_adv\n",
        "                else:\n",
        "                    loss = loss_rec\n",
        "            else: # two_stage training\n",
        "                if (args.second_stage_num > args.epochs-self.epoch): \n",
        "                    # last second_stage; flow loss_adv gradients\n",
        "                    loss = loss_rec + args.rho * loss_adv\n",
        "                else:\n",
        "                    loss = loss_rec\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            avg_meters['time'].update(time.time() - start_time)\n",
        "\n",
        "            # log progress\n",
        "            if (ix + 1) % args.log_interval == 0:\n",
        "                progress_meter.display(ix + 1)\n",
        "\n",
        "        progress_meter.display(len(self.train_loaders[0]))\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.models.eval()\n",
        "        # generate samples\n",
        "        inputs0, inputs1, outputs0, outputs1 = style_transfer(\n",
        "            encoder=self.models['encoder'],\n",
        "            generator=self.models['generator'],\n",
        "            text_path=args.val_text_file_path,\n",
        "            n_samples=args.n_samples\n",
        "        )\n",
        "\n",
        "        # display 10 samples for each\n",
        "        print('=' * 30 + '\\nnegative -> positive\\n' + '=' * 30 + '\\n')\n",
        "        for original, transfer in zip(inputs0[:10], outputs0[:10]):\n",
        "            print(original + ' -> ' + transfer + '\\n')\n",
        "        print('=' * 30 + '\\npositive -> negative\\n' + '=' * 30 + '\\n')\n",
        "        for original, transfer in zip(inputs1[:10], outputs1[:10]):\n",
        "            print(original + ' -> ' + transfer + '\\n')\n",
        "\n",
        "        print(\"Evaluation from {} samples\".format(args.n_samples))\n",
        "        fed = (calculate_frechet_distance(inputs1, outputs0)\n",
        "               + calculate_frechet_distance(inputs0, outputs1))\n",
        "        print('FED: {:.4f}'.format(fed))\n",
        "\n",
        "        loss, acc = calculate_accuracy(\n",
        "            self.clf,\n",
        "            outputs0 + outputs1,\n",
        "            torch.cat([\n",
        "                torch.ones(len(outputs0)),\n",
        "                torch.zeros(len(outputs1))\n",
        "            ]).long().to(args.device)\n",
        "        )\n",
        "        print('Loss: {:.4f}'.format(loss.item()))\n",
        "        print('Accuracy: {:.4f}\\n'.format(acc.item()))\n",
        "        return fed, loss.item(), acc.item()\n",
        "\n",
        "\n",
        "class Translator:\n",
        "    def __init__(self):\n",
        "        self.models = torch.load(args.ckpt_path)\n",
        "    def transfer(self):\n",
        "        self.models.eval()\n",
        "        if args.mode == 'interactive':\n",
        "            args.test_text_path = None\n",
        "        _, _, _, _ = style_transfer(\n",
        "            encoder=self.models['encoder'],\n",
        "            generator=self.models['generator'],\n",
        "            text_path=args.test_text_path,\n",
        "            n_samples=args.n_samples\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if args.mode == 'train':\n",
        "        trainer = Trainer()\n",
        "        printer = Metric_Printer('FED', 'Loss', 'Acc')\n",
        "        \n",
        "        loss_save = sys.maxsize\n",
        "        for _ in range(args.epochs):\n",
        "\n",
        "            trainer.train_epoch()\n",
        "            fed, loss, acc = trainer.evaluate()\n",
        "            if loss < loss_save:\n",
        "                loss_save = loss\n",
        "                print (\"saving model : \" + args.ckpt_path)\n",
        "                torch.save(trainer.models, args.ckpt_path)\n",
        "\n",
        "            printer.update(fed, loss, acc)\n",
        "        print(printer)\n",
        "    else:\n",
        "        translator = Translator()\n",
        "        translator.transfer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhUOA04zPuo5"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvUUWINfTU_0"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvN_QzNHSpCU"
      },
      "source": [
        "%cd mkdir tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UiTexsaOdLm"
      },
      "source": [
        "#options.py\n",
        "\n",
        "import sys\n",
        "import argparse\n",
        "import pprint\n",
        "import torch\n",
        "from utils import str2bool\n",
        "\n",
        "\n",
        "argparser = argparse.ArgumentParser(sys.argv[0])\n",
        "\n",
        "argparser.add_argument('--ckpt_path',\n",
        "                       required=True,\n",
        "                       help=\"path to save/load checkpoint\",\n",
        "                       type=str,\n",
        "                       default=\"/content/NLP_text-style-transfer/tmp\")\n",
        "argparser.add_argument('--clf_ckpt_path',\n",
        "                       help=\"path to load pretrained classifier\",\n",
        "                       type=str,\n",
        "                       default=\"/content/NLP_text-style-transfer/bert_pretrained/classifier.py\")\n",
        "\n",
        "# dataloading\n",
        "argparser.add_argument('--dataset',\n",
        "                       type=str,\n",
        "                       choices=['yelp', 'nsmc'],\n",
        "                       default=None)\n",
        "argparser.add_argument('--text_file_path',\n",
        "                       type=str)\n",
        "argparser.add_argument('--val_text_file_path',\n",
        "                       type=str)\n",
        "argparser.add_argument('--batch_size',\n",
        "                       type=int,\n",
        "                       default=64)\n",
        "argparser.add_argument('--max_seq_length',\n",
        "                       type=int,\n",
        "                       default=64)\n",
        "argparser.add_argument('--num_workers',\n",
        "                       type=int,\n",
        "                       default=4)\n",
        "\n",
        "# architecture\n",
        "argparser.add_argument('--dim_y',\n",
        "                       type=int,\n",
        "                       default=200)\n",
        "argparser.add_argument('--dim_z',\n",
        "                       type=int,\n",
        "                       default=500)\n",
        "argparser.add_argument('--dim_emb',\n",
        "                       type=int,\n",
        "                       default=768)\n",
        "argparser.add_argument('--filter_sizes',\n",
        "                       type=int,\n",
        "                       nargs='+',\n",
        "                       default=[1, 2, 3, 4, 5])\n",
        "argparser.add_argument('--n_filters',\n",
        "                       type=int,\n",
        "                       default=128)\n",
        "\n",
        "# learning\n",
        "argparser.add_argument('--epochs',\n",
        "                       type=int,\n",
        "                       default=20)\n",
        "argparser.add_argument('--weight_decay',\n",
        "                       type=float,\n",
        "                       default=0.0)\n",
        "argparser.add_argument('--max_grad_norm',\n",
        "                       type=float,\n",
        "                       default=1.0)\n",
        "argparser.add_argument('--lr',\n",
        "                       type=float,\n",
        "                       default=5e-4)\n",
        "argparser.add_argument('--disc_lr',\n",
        "                       type=float,\n",
        "                       default=5e-5)\n",
        "argparser.add_argument(\"--temperature\",\n",
        "                       type=float,\n",
        "                       default=0.1)\n",
        "argparser.add_argument('--use_gumbel',\n",
        "                       default=True,\n",
        "                       type=str2bool)\n",
        "argparser.add_argument('--rho',  # loss_rec + rho * loss_adv\n",
        "                       type=float,\n",
        "                       default=1)\n",
        "argparser.add_argument('--two_stage',\n",
        "                        type=str2bool,\n",
        "                        default=False)\n",
        "argparser.add_argument('--second_stage_num',\n",
        "                        type=int,\n",
        "                        default=2)\n",
        "argparser.add_argument('--gan_type',\n",
        "                       default='vanilla',\n",
        "                       choices=['vanilla', 'wgan-gp', 'lsgan'])\n",
        "argparser.add_argument('--gp_weight',\n",
        "                       default=1.0,\n",
        "                       type=float)\n",
        "argparser.add_argument('--log_interval',\n",
        "                       default=100,\n",
        "                       type=int)\n",
        "argparser.add_argument('--language',\n",
        "                       default='ko',\n",
        "                       choices=['ko', 'en'])\n",
        "argparser.add_argument('--threshold',\n",
        "                       type=float,\n",
        "                       default=None)\n",
        "\n",
        "# testing\n",
        "argparser.add_argument('--mode',\n",
        "                        help='train or transfer',\n",
        "                        choices=['train', 'transfer', 'interactive'],\n",
        "                        default='train',\n",
        "                        type=str)\n",
        "argparser.add_argument('--test_text_path',\n",
        "                       help='path to text file whose each line contains one sentence',\n",
        "                       default=None)\n",
        "argparser.add_argument('--transfer_to', #only use in interactive transfer\n",
        "                       default=1,\n",
        "                       type=int,\n",
        "                       choices=[0, 1])\n",
        "argparser.add_argument('--n_samples',\n",
        "                       default=1000,\n",
        "                       type=int)\n",
        "argparser.add_argument('--transfer_max_len',\n",
        "                       default=64,\n",
        "                       type=int)\n",
        "argparser.add_argument(\"--transfer_result_save_path\",\n",
        "                       default=None,\n",
        "                       help=\"path to save transfer result\")\n",
        "\n",
        "# others\n",
        "argparser.add_argument(\"--cuda_device\",\n",
        "                       type=int,\n",
        "                       default=0)\n",
        "argparser.add_argument(\"--load_ckpt\", # Tobe deleted after experimenting\n",
        "                       type=str,\n",
        "                       default=\"ckpts/wgan_no_threshold.pt\")\n",
        "\n",
        "args = argparser.parse_args()\n",
        "\n",
        "args.device = torch.device(\n",
        "    'cuda:{}'.format(args.cuda_device) if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "# dataset presets\n",
        "if args.dataset == 'yelp':\n",
        "    args.language = 'en'\n",
        "    args.text_file_path = 'data/yelp/yelp.sentiment.train'\n",
        "    args.val_text_file_path = 'data/yelp/yelp.sentiment.val'\n",
        "    args.test_text_path = 'data/yelp/yelp.sentiment.test'\n",
        "    args.clf_ckpt_path = 'checkpoints/yelp_clf.pt'\n",
        "elif args.dataset == 'nsmc':\n",
        "    args.language = 'ko'\n",
        "    args.text_file_path = 'data/nsmc/ratings_train.txt'\n",
        "    args.val_text_file_path = 'data/nsmc/ratings_test.txt'\n",
        "    args.test_text_path = 'data/nsmc/ratings_test.txt'\n",
        "    args.clf_ckpt_path = 'checkpoints/nsmc_clf.pt'\n",
        "elif args.dataset is None:\n",
        "    assert args.text_file_path is not None\n",
        "    assert args.val_text_file_path is not None\n",
        "    assert args.test_text_path is not None\n",
        "\n",
        "print('------------------------------------------------')\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(vars(args))\n",
        "print('------------------------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuYyhiU9L9Dc",
        "outputId": "a0743fb9-4532-406b-a0b3-d33e328f05f3"
      },
      "source": [
        "%cd /content/NLP_text-style-transfer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NLP_text-style-transfer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp9dJLJFMGqS"
      },
      "source": [
        "%mkdir checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9N7KXrTMPAz",
        "outputId": "86ba0d40-cee2-491e-c805-a20aa98d4760"
      },
      "source": [
        "!python train.py --ckpt_path \"checkpoints\" --clf_ckpt_path \"checkpoints\" --dataset yelp --text_file_path \"data\" --val_text_file_path \"data\" --gan_type vanilla --language en --threshold 0.1 --mode interactive --test_text_path \"data\" --transfer_to 0\n",
        "       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------\n",
            "{   'batch_size': 64,\n",
            "    'ckpt_path': 'checkpoints',\n",
            "    'clf_ckpt_path': 'checkpoints/yelp_clf.pt',\n",
            "    'cuda_device': 0,\n",
            "    'dataset': 'yelp',\n",
            "    'device': device(type='cpu'),\n",
            "    'dim_emb': 768,\n",
            "    'dim_y': 200,\n",
            "    'dim_z': 500,\n",
            "    'disc_lr': 5e-05,\n",
            "    'epochs': 20,\n",
            "    'filter_sizes': [1, 2, 3, 4, 5],\n",
            "    'gan_type': 'vanilla',\n",
            "    'gp_weight': 1.0,\n",
            "    'language': 'en',\n",
            "    'load_ckpt': 'ckpts/wgan_no_threshold.pt',\n",
            "    'log_interval': 100,\n",
            "    'lr': 0.0005,\n",
            "    'max_grad_norm': 1.0,\n",
            "    'max_seq_length': 64,\n",
            "    'mode': 'interactive',\n",
            "    'n_filters': 128,\n",
            "    'n_samples': 1000,\n",
            "    'num_workers': 4,\n",
            "    'rho': 1,\n",
            "    'second_stage_num': 2,\n",
            "    'temperature': 0.1,\n",
            "    'test_text_path': 'data/yelp/yelp.sentiment.test',\n",
            "    'text_file_path': 'data/yelp/yelp.sentiment.train',\n",
            "    'threshold': 0.1,\n",
            "    'transfer_max_len': 64,\n",
            "    'transfer_result_save_path': None,\n",
            "    'transfer_to': 0,\n",
            "    'two_stage': False,\n",
            "    'use_gumbel': True,\n",
            "    'val_text_file_path': 'data/yelp/yelp.sentiment.val',\n",
            "    'weight_decay': 0.0}\n",
            "------------------------------------------------\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 279, in <module>\n",
            "    translator = Translator()\n",
            "  File \"train.py\", line 248, in __init__\n",
            "    self.models = torch.load(args.ckpt_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 594, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 211, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "IsADirectoryError: [Errno 21] Is a directory: 'checkpoints'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkKZWMwSOk1S"
      },
      "source": [
        "#utils.py\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "import numpy as np\n",
        "import scipy.linalg\n",
        "\n",
        "URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "\n",
        "def save_model(model, ckpt_path):\n",
        "    pardir = os.path.dirname(os.path.abspath(ckpt_path))\n",
        "    if not os.path.isdir(pardir):\n",
        "        os.makedirs(pardir, exist_ok=True)\n",
        "    torch.save(\n",
        "        {'model_state_dict': model.state_dict()},\n",
        "        ckpt_path\n",
        "    )\n",
        "\n",
        "\n",
        "def download_google(file_id, filename):\n",
        "    pardir = os.path.dirname(os.path.abspath(filename))\n",
        "    if not os.path.isdir(pardir):\n",
        "        os.makedirs(pardir, exist_ok=True)\n",
        "    if not os.path.isfile(filename):\n",
        "        # Request file from URL\n",
        "        session = requests.Session()\n",
        "        response = session.get(URL, params={'id': file_id}, stream=True)\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                params = {'id': file_id, 'confirm': value}\n",
        "                response = session.get(URL, params=params, stream=True)\n",
        "\n",
        "        # Download file\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "\n",
        "def str2bool(s):\n",
        "    if s.lower() in ['true', 't', 'yes']:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def covariance(m, rowvar=True, inplace=False):\n",
        "    '''Estimate a covariance matrix given data.\n",
        "\n",
        "    Covariance indicates the level to which two variables vary together.\n",
        "    If we examine N-dimensional samples, `X = [x_1, x_2, ... x_N]^T`,\n",
        "    then the covariance matrix element `C_{ij}` is the covariance of\n",
        "    `x_i` and `x_j`. The element `C_{ii}` is the variance of `x_i`.\n",
        "\n",
        "    Args:\n",
        "        m: A 1-D or 2-D array containing multiple variables and observations.\n",
        "            Each row of `m` represents a variable, and each column a single\n",
        "            observation of all those variables.\n",
        "        rowvar: If `rowvar` is True, then each row represents a\n",
        "            variable, with observations in the columns. Otherwise, the\n",
        "            relationship is transposed: each column represents a variable,\n",
        "            while the rows contain observations.\n",
        "\n",
        "    Returns:\n",
        "        The covariance matrix of the variables.\n",
        "    '''\n",
        "    if m.dim() > 2:\n",
        "        raise ValueError('m has more than 2 dimensions')\n",
        "    if m.dim() < 2:\n",
        "        m = m.view(1, -1)\n",
        "    if not rowvar and m.size(0) != 1:\n",
        "        m = m.t()\n",
        "    # m = m.type(torch.double)  # uncomment this line if desired\n",
        "    fact = 1.0 / (m.size(1) - 1)\n",
        "    if inplace:\n",
        "        m -= torch.mean(m, dim=1, keepdim=True)\n",
        "    else:\n",
        "        m = m - torch.mean(m, dim=1, keepdim=True)\n",
        "    mt = m.t()  # if complex: mt = m.t().conj()\n",
        "    return fact * m.matmul(mt).squeeze()\n",
        "\n",
        "\n",
        "class MatrixSquareRoot(Function):\n",
        "    \"\"\"Square root of a positive definite matrix.\n",
        "    NOTE: matrix square root is not differentiable for matrices with\n",
        "          zero eigenvalues.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        m = input.detach().cpu().numpy().astype(np.float_)\n",
        "        sqrtm = torch.from_numpy(scipy.linalg.sqrtm(m).real).to(input)\n",
        "        ctx.save_for_backward(sqrtm)\n",
        "        return sqrtm\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        grad_input = None\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            sqrtm, = ctx.saved_tensors\n",
        "            sqrtm = sqrtm.data.cpu().numpy().astype(np.float_)\n",
        "            gm = grad_output.data.cpu().numpy().astype(np.float_)\n",
        "\n",
        "            # Given a positive semi-definite matrix X,\n",
        "            # since X = X^{1/2}X^{1/2}, we can compute the gradient of the\n",
        "            # matrix square root dX^{1/2} by solving the Sylvester equation:\n",
        "            # dX = (d(X^{1/2})X^{1/2} + X^{1/2}(dX^{1/2}).\n",
        "            grad_sqrtm = scipy.linalg.solve_sylvester(sqrtm, sqrtm, gm)\n",
        "\n",
        "            grad_input = torch.from_numpy(grad_sqrtm).to(grad_output)\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "def sqrtm(m):\n",
        "    return MatrixSquareRoot.apply(m)\n",
        "\n",
        "class Metric_Printer(object):\n",
        "    def __init__(self, *meters):\n",
        "        self.meters = list(meters)\n",
        "        self.values = []\n",
        "    def update(self, *values):\n",
        "        self.values.append(values)\n",
        "    def __str__(self):\n",
        "        names = '{:^6}'.format('epoch')\n",
        "        val = ''\n",
        "        for i in range(len(self.meters)):\n",
        "            names += '{:^10}'.format(self.meters[i]) \n",
        "        names += '\\n'\n",
        "        for i in range(len(self.values)):\n",
        "            val += '[{:^6}]'.format(i+1)\n",
        "            for j in range(len(self.values[i])):\n",
        "                val += '{:^10.4f}'.format(self.values[i][j])\n",
        "            val += '\\n' \n",
        "        return names + val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ES5UxaZOqnh"
      },
      "source": [
        "#dataloader.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from bert_pretrained.tokenizer import bert_tokenizer\n",
        "from options import args\n",
        "\n",
        "\n",
        "class SentimentAnalysis(Dataset):\n",
        "    def __init__(self, txt_path, maxlen=256):\n",
        "        self.maxlen = maxlen\n",
        "        self.texts = []\n",
        "        self.labels = []\n",
        "        with open(txt_path) as fr:\n",
        "            fr.readline()  # header line\n",
        "            for line in fr:\n",
        "                line = line.strip().split('\\t')  # expects tsv format\n",
        "                self.texts.append(line[1])\n",
        "                self.labels.append(int(line[2]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "        # add [CLS] and [SEP] tokens\n",
        "        text_tokens = bert_tokenizer.encode(text, add_special_tokens=True)\n",
        "        text_tokens = (text_tokens[:min(len(text_tokens), self.maxlen) - 1]\n",
        "                       + text_tokens[-1:])  # must always include [SEP]\n",
        "        return torch.LongTensor(text_tokens), labels\n",
        "\n",
        "\n",
        "class StyleTransfer(Dataset):\n",
        "    def __init__(self, txt_path, maxlen=256, label=1):\n",
        "        assert (bert_tokenizer.bos_token_id is not None\n",
        "                and bert_tokenizer.eos_token_id is not None)\n",
        "        self.maxlen = maxlen\n",
        "        self.texts = []\n",
        "        self.labels = []\n",
        "        with open(txt_path) as fr:\n",
        "            fr.readline()  # header line\n",
        "            for line in fr:\n",
        "                line = line.strip().split('\\t')  # expects tsv format\n",
        "                if int(line[2]) == label:\n",
        "                    self.texts.append(line[1])\n",
        "                    self.labels.append(int(line[2]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "        # exclude [CLS] and [SEP]\n",
        "        text_tokens = bert_tokenizer.encode(text, add_special_tokens=False)\n",
        "        text_tokens = ([bert_tokenizer.bos_token_id]\n",
        "                       + text_tokens[:self.maxlen - 2]\n",
        "                       + [bert_tokenizer.eos_token_id])\n",
        "        return torch.LongTensor(text_tokens), labels\n",
        "\n",
        "\n",
        "def get_dataloader_for_classification(txt_path, shuffle=True, drop_last=True):\n",
        "    def collate_fn(data):\n",
        "        input_ids, class_labels = zip(*data)\n",
        "\n",
        "        input_ids_lens = [len(inp) for inp in input_ids]\n",
        "        input_ids_maxlen = max(input_ids_lens)\n",
        "\n",
        "        padded_input_ids = torch.zeros(len(input_ids), input_ids_maxlen, dtype=int)  # Long Tensor with (batch_size, maxlen)\n",
        "        attention_mask = torch.zeros(len(input_ids), input_ids_maxlen, dtype=int)  # Long Tensor with (batch_size, maxlen), 1 if not padded 0 if padded\n",
        "\n",
        "        for ix in range(len(input_ids)):\n",
        "            padded_input_ids[ix, :input_ids_lens[ix]] = input_ids[ix]\n",
        "            attention_mask[ix, :input_ids_lens[ix]] = 1\n",
        "\n",
        "        class_labels = torch.LongTensor(class_labels)  # (batch_size, num_classes)\n",
        "\n",
        "        return padded_input_ids, attention_mask, class_labels\n",
        "\n",
        "    return DataLoader(\n",
        "        SentimentAnalysis(txt_path, maxlen=args.max_seq_length),\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=drop_last,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "\n",
        "def get_dataloader_for_style_transfer(txt_path, shuffle=True, drop_last=True):\n",
        "    def collate_fn(data):\n",
        "        input_ids, class_labels = zip(*data)\n",
        "\n",
        "        input_ids_lens = [len(inp) for inp in input_ids]\n",
        "        input_ids_maxlen = max(input_ids_lens)\n",
        "        # Long Tensor with (maxlen, batch_size)\n",
        "        padded_input_ids = torch.zeros(input_ids_maxlen, len(input_ids), dtype=int)\n",
        "        padded_input_ids = padded_input_ids.fill_(bert_tokenizer.pad_token_id)\n",
        "\n",
        "        for ix in range(len(input_ids)):\n",
        "            padded_input_ids[:input_ids_lens[ix], ix] = input_ids[ix]\n",
        "\n",
        "        class_labels = torch.FloatTensor(class_labels)  # (batch_size, num_classes)\n",
        "\n",
        "        return padded_input_ids, input_ids_lens, class_labels\n",
        "\n",
        "    dataloader0 = DataLoader(\n",
        "        StyleTransfer(txt_path, maxlen=args.max_seq_length, label=0),\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=drop_last,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    dataloader1 = DataLoader(\n",
        "        StyleTransfer(txt_path, maxlen=args.max_seq_length, label=1),\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=drop_last,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    return dataloader0, dataloader1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4XFD1IVOtZr"
      },
      "source": [
        "#eval.py\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from bert_pretrained import extract_features, bert_tokenizer\n",
        "from utils import covariance, sqrtm\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"PyTorch implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    Stable version by Dougal J. Sutherland.\n",
        "    Params:\n",
        "    -- mu1   : Numpy array containing the activations of a layer of the\n",
        "               inception net (like returned by the function 'get_predictions')\n",
        "               for generated samples.\n",
        "    -- mu2   : The sample mean over activations, precalculated on an\n",
        "               representative data set.\n",
        "    -- sigma1: The covariance matrix over activations for generated samples.\n",
        "    -- sigma2: The covariance matrix over activations, precalculated on an\n",
        "               representative data set.\n",
        "    Returns:\n",
        "    --   : The Frechet Distance.\n",
        "    \"\"\"\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Two mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Two covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    # Product might be almost singular\n",
        "    # NOTE: the matrix square root is forced to be real\n",
        "    covmean = sqrtm(sigma1 @ sigma2)\n",
        "    if not torch.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = torch.eye(sigma1.size(0)) * eps\n",
        "        covmean = sqrtm((sigma1 + offset) @ (sigma2 + offset))\n",
        "\n",
        "    return (diff @ diff + torch.trace(sigma1) + torch.trace(sigma2)\n",
        "            - 2 * torch.trace(covmean))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_frechet_distance(text1, text2, verbose=False):\n",
        "    first = []\n",
        "    for line in tqdm(text1, disable=not verbose):\n",
        "        first.append(extract_features(line.strip()))\n",
        "    first = torch.stack(first, dim=0)\n",
        "    mu1 = torch.mean(first, dim=0)\n",
        "    sigma1 = covariance(first, rowvar=False)\n",
        "\n",
        "    second = []\n",
        "    for line in tqdm(text2, disable=not verbose):\n",
        "        second.append(extract_features(line.strip()))\n",
        "    second = torch.stack(second, dim=0)\n",
        "    mu2 = torch.mean(second, dim=0)\n",
        "    sigma2 = covariance(second, rowvar=False)\n",
        "\n",
        "    return _frechet_distance(mu1, sigma1, mu2, sigma2)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_accuracy(clf, text, labels, verbose=False):\n",
        "    outputs = []\n",
        "    for line in tqdm(text, disable=not verbose):\n",
        "        inputs = bert_tokenizer(\n",
        "            line.strip(),\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt',\n",
        "            padding=True\n",
        "        ).to(labels.device)\n",
        "        outputs.append(clf(**inputs))\n",
        "    outputs = torch.cat(outputs, dim=0)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "    acc = (preds == labels).float().mean()\n",
        "\n",
        "    return loss, acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N023-lDFOxwp"
      },
      "source": [
        "#model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding, dim_y, dim_z):\n",
        "        \"\"\"\n",
        "        Required parameters:\n",
        "            embedding: nn.Embedding\n",
        "            dim_y: hyperparam\n",
        "            dim_z: hyperparam\n",
        "\n",
        "        구성요소:\n",
        "            Embedding Layer\n",
        "            Fully connected Layer (to get latent variable `y`)\n",
        "            unidirectional GRU with layer 1\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(1, dim_y)  # output: (batch_size, dim_y)\n",
        "        self.register_buffer(\"init_z\", torch.zeros(dim_z, requires_grad=False))\n",
        "        self.embed = embedding\n",
        "\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed.embedding_dim,\n",
        "            dim_y + dim_z,\n",
        "            num_layers=1\n",
        "        )\n",
        "        self.dim_y = dim_y\n",
        "\n",
        "    def forward(self, labels, src, src_len):\n",
        "        \"\"\"\n",
        "        labels: torch.LongTensor with shape (batch_size,)\n",
        "        src: torch.LongTensor with shape (max_seq_len, batch_size)\n",
        "        src_len: list of seq lengths\n",
        "        \"\"\"\n",
        "        labels = labels.unsqueeze(-1)  # (batch_size, 1), 엔코더 밖에서 해줘도 괜찮을 듯\n",
        "        src = self.embed(src)  # (max_seq_len, batch_size, embed_dim)\n",
        "        packed_embed = nn.utils.rnn.pack_padded_sequence(  # input to rnn\n",
        "            src,\n",
        "            src_len,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # initial hidden state of the encoder: concat (y, z)\n",
        "        # [ batch size: src.shape[1] , dim_z ]\n",
        "        init_z = self.init_z.repeat(src.shape[1], 1)\n",
        "        init_hidden = torch.cat((self.fc(labels), init_z), -1)\n",
        "\n",
        "        _, hidden = self.rnn(packed_embed, init_hidden.unsqueeze(0))\n",
        "        # hidden : hidden_state of the final time step\n",
        "        hidden = hidden.squeeze(0)  # (batch_size, hidden_dim)\n",
        "        z = hidden[:, self.dim_y:]  # (batch_size, dim_z)\n",
        "        return z\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, embeddings, dim_y, dim_z, temperature,\n",
        "                 bos_token_id=8002, use_gumbel=True):\n",
        "        \"\"\"\n",
        "        Required parameters:\n",
        "            embedding: nn.Embedding()\n",
        "            dim_y: .\n",
        "            dim_z: .\n",
        "            temperature: refer to paper\n",
        "            bos_token_id: tokenizer.bos_token_id\n",
        "\n",
        "        Components:\n",
        "            Fully connected Layer (to get latent `y`)\n",
        "            Word Embedding\n",
        "            Unidirectional GRU (layer=1)\n",
        "            Fully connected Layer (prediction)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dim_h = dim_y + dim_z\n",
        "\n",
        "        self.embed = embeddings  # type(embeddings) = nn.Embedding\n",
        "        self.register_buffer(\n",
        "            \"bos_token_id\",\n",
        "            torch.tensor([bos_token_id], dtype=int)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(1, dim_y)  # latent `y`\n",
        "        # The hidden state's dimension: dim_y + dim_z\n",
        "        self.rnn = nn.GRU(self.embed.embedding_dim, self.dim_h, num_layers=1)\n",
        "        self.fc_out = nn.Linear(self.dim_h, self.embed.num_embeddings)  # prediction\n",
        "        self.use_gumbel = use_gumbel\n",
        "\n",
        "    def transfer(self, z, label_to_transfer_to, eos_token_id=8003, max_len=64,\n",
        "                 top_k=5):\n",
        "        label = label_to_transfer_to\n",
        "        assert z.size(0) == 1 and label.size(0) == 1\n",
        "        label = label.unsqueeze(-1)\n",
        "\n",
        "        h0 = torch.cat((self.fc(label), z), -1)  # (1, dim_h)\n",
        "\n",
        "        input = self.embed(self.bos_token_id).repeat(1, 1).unsqueeze(0)  # (1, 1, embed_dim)\n",
        "        hidden = h0.unsqueeze(0)  # (1, 1, dim_h)\n",
        "\n",
        "        predictions = []\n",
        "        for t in range(max_len):\n",
        "            output, hidden = self.rnn(input, hidden)\n",
        "            prediction = self.fc_out(output)  # (1, 1, num_embedding)\n",
        "\n",
        "            top_k_logits = prediction.topk(top_k).values\n",
        "            top_k_indices = prediction.topk(top_k).indices\n",
        "\n",
        "            sample = Categorical(logits=top_k_logits).sample().item()\n",
        "            prediction = top_k_indices[:, :, sample]  # (1, 1)\n",
        "            predictions.append(prediction.item())\n",
        "            if prediction == eos_token_id:\n",
        "                break\n",
        "\n",
        "            input = self.embed(prediction)  # (1, 1, embed_dim)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def forward(self, z, labels, src, src_len, transfered=True):\n",
        "        \"\"\"\n",
        "        Required Parameters\n",
        "            src: original sentence [seq_len, batch_size]\n",
        "            src_len: original sentence len [batch_size]\n",
        "\n",
        "        * using gumbel_softmax\n",
        "\n",
        "        Returns:\n",
        "            outpus: to feed to discriminator\n",
        "            predictions: get loss_rec\n",
        "        \"\"\"\n",
        "        labels = labels.unsqueeze(-1)  # (batch_size, 1)\n",
        "\n",
        "        # placeholders for outputs and prediction tensors\n",
        "        outputs = []\n",
        "        predictions = []\n",
        "\n",
        "        if transfered:\n",
        "            # Feed previous decoding\n",
        "            h0 = torch.cat((self.fc(1-labels), z), -1)  # h0_transfered\n",
        "\n",
        "            input = self.embed(self.bos_token_id).repeat(src.shape[1], 1)\n",
        "            input = input.unsqueeze(0)      # [1, batch_size, embed_dim]\n",
        "            hidden = h0.unsqueeze(0)        # [1, batch_size, hidden_size]\n",
        "            for t in range(1, max(src_len)):\n",
        "                output, hidden = self.rnn(input, hidden)\n",
        "                outputs.append(output)\n",
        "                prediction = self.fc_out(output)\n",
        "                predictions.append(prediction)\n",
        "\n",
        "                # 원본코드의 softsample_word를 참조\n",
        "                T = self.temperature\n",
        "                if self.use_gumbel:\n",
        "                    input = torch.matmul(\n",
        "                        F.gumbel_softmax(prediction / T, dim=-1),\n",
        "                        self.embed.weight\n",
        "                    )\n",
        "                else:\n",
        "                    input = torch.matmul(\n",
        "                        F.softmax(prediction / T, dim=-1),\n",
        "                        self.embed.weight\n",
        "                    )\n",
        "\n",
        "        else:\n",
        "            # Teacher Forcing\n",
        "            h0 = torch.cat((self.fc(labels), z), -1)  # h0_original\n",
        "            input = self.embed(src[0]).unsqueeze(0)\n",
        "            hidden = h0.unsqueeze(0)  # [1, batch_size, hidden_size]\n",
        "            for t in range(1, max(src_len)):\n",
        "                output, hidden = self.rnn(input, hidden)\n",
        "                outputs.append(output)\n",
        "                prediction = self.fc_out(output)\n",
        "                predictions.append(prediction)\n",
        "                input = self.embed(src[t]).unsqueeze(0)\n",
        "\n",
        "        outputs = torch.cat([h0.unsqueeze(0)] + outputs, 0)  # according to the paper you need h0 in the sequence to feed the discriminator\n",
        "        predictions = torch.cat(predictions, 0)\n",
        "        # outputs = [ 1 + max_seq_len, batch_size, hidden_size]\n",
        "        # predictions = [max_seq_len, batch_size, hidden_size]\n",
        "        return outputs, predictions\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, dim_h, n_filters, filter_sizes, output_dim=1,\n",
        "                 dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.cnn = TextCNN(dim_h, n_filters, filter_sizes, output_dim=1,\n",
        "                           dropout=dropout)\n",
        "\n",
        "    def forward(self, h_sequence):\n",
        "        # h_sequence: [seq_len, batch_size, hidden_dim]\n",
        "        return self.cnn(h_sequence.transpose(0, 1))\n",
        "\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, dim_h, n_filters, filter_sizes, output_dim, dropout):\n",
        "        # 원본 코드 상의 output_dim은 1\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1,\n",
        "                      out_channels=n_filters,\n",
        "                      kernel_size=(fs, dim_h))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, hiddens):\n",
        "        # don't forget the permutation\n",
        "        # hiddens = [batch_size, hiddens seq len, dim_h]\n",
        "        hiddens = hiddens.unsqueeze(1)\n",
        "        # hiddens = [batch_size, 1, hiddens seq len, dim_h]\n",
        "\n",
        "        conved = [\n",
        "            F.leaky_relu(conv(hiddens)).squeeze(3) for conv in self.convs\n",
        "        ]\n",
        "        # conved[n] = [batch size, n_filters, dim_h - filter_sizes[n] + 1]\n",
        "\n",
        "        pooled = [\n",
        "            F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved\n",
        "        ]\n",
        "        # pooled[n] = [batch size, n_filters]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
        "        return self.fc(cat)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cZ_1UcWOzJZ"
      },
      "source": [
        "#loss.py\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def gradient_penalty(real_data, gen_data, disc):\n",
        "    \"\"\"code modified from https://github.com/EmilienDupont/wgan-gp\"\"\"\n",
        "    # Calculate interpolation\n",
        "    batch_size = real_data.size(1)\n",
        "    alpha = torch.rand(1, batch_size, 1).to(real_data)\n",
        "    alpha = alpha.expand_as(real_data)\n",
        "    interpolated = alpha * real_data.data + (1 - alpha) * gen_data.data\n",
        "    interpolated.requires_grad = True\n",
        "\n",
        "    # Calculate probability of interpolated examples\n",
        "    prob_interpolated = disc(interpolated)\n",
        "\n",
        "    # Calculate gradients of probabilities with respect to examples\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=prob_interpolated,\n",
        "        inputs=interpolated,\n",
        "        grad_outputs=torch.ones_like(prob_interpolated),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )\n",
        "    # Gradients have shape (seq_len, batch_size, hidden_dim)\n",
        "    # so flatten to easily take norm per example in batch\n",
        "    gradients = gradients[0].transpose(1, 0).view(batch_size, -1)\n",
        "\n",
        "    # Derivatives of the gradient close to 0 can cause problems because of\n",
        "    # the square root, so manually calculate norm and add epsilon\n",
        "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "\n",
        "    # Return gradient penalty\n",
        "    return ((gradients_norm - 1) ** 2).mean()\n",
        "\n",
        "\n",
        "def wgan_disc(data, labels):\n",
        "    probs = torch.sigmoid(data)\n",
        "    return (-probs * labels + probs * (1 - labels)).mean()\n",
        "\n",
        "\n",
        "def wgan_gen(data, labels):\n",
        "    probs = torch.sigmoid(data)\n",
        "    return (-probs * labels).mean()\n",
        "\n",
        "\n",
        "def loss_fn(gan_type='vanilla', disc=True):\n",
        "    if gan_type == 'vanilla':\n",
        "        return F.binary_cross_entropy_with_logits\n",
        "    elif gan_type == 'lsgan':\n",
        "        return F.mse_loss\n",
        "    elif gan_type == 'wgan-gp':\n",
        "        return wgan_disc if disc else wgan_gen\n",
        "    else:\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEiE-IllPK_V"
      },
      "source": [
        "#bert_pretrained_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "from bert_pretrained.tokenizer import bert_tokenizer\n",
        "from options import args\n",
        "\n",
        "\n",
        "if args.language == 'ko':\n",
        "    model_type = 'monologg/kobert'\n",
        "else:\n",
        "    model_type = 'bert-base-cased'\n",
        "BERT = BertModel.from_pretrained(model_type).to(args.device)\n",
        "\n",
        "\n",
        "def get_bert_word_embedding():\n",
        "    num_embeddings = (bert_tokenizer.vocab_size\n",
        "                      + len(bert_tokenizer.get_added_vocab()))\n",
        "    embed_dim = BERT.embeddings.word_embeddings.embedding_dim\n",
        "\n",
        "    # need to add embedding for bos and eos token\n",
        "    embedding = nn.Embedding(\n",
        "        num_embeddings,\n",
        "        embed_dim,\n",
        "        padding_idx=bert_tokenizer.pad_token_id\n",
        "    )\n",
        "    embedding.weight.data[:bert_tokenizer.vocab_size].copy_(\n",
        "        BERT.embeddings.word_embeddings.weight.data\n",
        "    )\n",
        "    return embedding\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_features(text):\n",
        "    inputs = bert_tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors='pt',\n",
        "        padding=True\n",
        "    ).to(args.device)\n",
        "    features = BERT(**inputs)[0]\n",
        "    # sentence embedding = mean of word embedding\n",
        "    return features.squeeze(0).mean(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}